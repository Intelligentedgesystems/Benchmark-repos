{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/tools/inference.py', 'lineno': 0, 'args': {}, 'lines': ['"""', 'import argparse', 'from yolov3_pytorch.engine.inferencer import Inferencer'], 'executed_lines': {17, 19, 14}, 'executed_function_lines': {0}, 'extra_calls': 0}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .evaler import *'], 'executed_lines': {14}, 'executed_function_lines': {0}, 'extra_calls': 0}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/evaler.py', 'lineno': 0, 'args': {}, 'lines': ['import json', 'from pathlib import Path', 'from typing import Dict', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from pycocotools.coco import COCO', 'from pycocotools.cocoeval import COCOeval', 'from torch import nn', 'from torchvision.ops import boxes', 'from tqdm import tqdm', 'from yolov3_pytorch.data.base import BaseDatasets'], 'executed_lines': {14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 27}, 'executed_function_lines': {0}, 'extra_calls': 0}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .base import *'], 'executed_lines': {14}, 'executed_function_lines': {0}, 'extra_calls': 0}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/base.py', 'lineno': 0, 'args': {}, 'lines': ['"""', 'import glob', 'import os', 'import random', 'from pathlib import Path', 'from typing import Any, Tuple, List', 'import cv2', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from PIL import ExifTags, Image', 'from skimage import io', 'from torch import Tensor', 'from torchvision.transforms import functional as F_vision', 'from tqdm import tqdm', 'from yolov3_pytorch.utils.common import xywh2xyxy, xyxy2xywh'], 'executed_lines': {33, 14, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31}, 'executed_function_lines': {0}, 'extra_calls': 0}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .autochor import *'], 'executed_lines': {14}, 'executed_function_lines': {0}, 'extra_calls': 0}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/autochor.py', 'lineno': 0, 'args': {}, 'lines': ['import numpy as np', 'import torch', 'from scipy.cluster.vq import kmeans', 'from tqdm import tqdm', 'from .metrics.iou import wh_iou'], 'executed_lines': {14, 15, 16, 17, 19}, 'executed_function_lines': {0}, 'extra_calls': 0}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/metrics/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .ap import ap_per_class, compute_ap'], 'executed_lines': {14}, 'executed_function_lines': {0}, 'extra_calls': 0}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/metrics/ap.py', 'lineno': 0, 'args': {}, 'lines': ['import numpy as np', '"compute_ap", "ap_per_class",', '__all__ = [', 'def compute_ap(recall: np.ndarray, precision: np.ndarray) -> float:', 'def ap_per_class(tp: np.ndarray, conf: np.ndarray, pred_cls: np.ndarray, target_cls: np.ndarray):'], 'executed_lines': {45, 14, 16, 17, 21}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/metrics/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .ap import ap_per_class, compute_ap', 'from .iou import box_iou, bbox_iou, wh_iou', 'import numpy as np', 'import torch', 'from torch import Tensor', '__all__ = [', 'def box_iou(box1: Tensor or np.ndarray, box2: Tensor or np.ndarray) -> Tensor or np.ndarray:', 'def bbox_iou(box1, box2, x1y1x2y2=True, g_iou=False, d_iou=False, c_iou=False):', 'def wh_iou(wh1, wh2):'], 'executed_lines': {14, 15, 16, 17, 18, 112, 20, 25, 61}, 'executed_function_lines': {0}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/autochor.py', 'lineno': 0, 'args': {}, 'lines': ['import numpy as np', 'import torch', 'from scipy.cluster.vq import kmeans', 'from tqdm import tqdm', 'from .metrics.iou import wh_iou', '"kmean_anchors",', '__all__ = [', 'path: str = "./data/voc0712/train.txt",', 'num_anchor: int = 9,', 'image_size: tuple = (640, 640),', 'iou_thresh: float = 0.25,', 'gen: int = 1000,', 'def kmean_anchors(', ') -> np.ndarray:'], 'executed_lines': {32, 14, 15, 16, 17, 19, 21, 22, 26, 27, 28, 29, 30, 31}, 'executed_function_lines': {0}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .autochor import *', 'from .common import *', 'import numpy as np', 'import torch', 'from torch import Tensor', 'try:', 'import accimage', 'except ImportError:', 'accimage = None', '__all__ = [', 'def clip_coords(boxes: Tensor, image_shape: tuple) -> Tensor:', 'def coco80_to_coco91_class() -> list:', 'def labels_to_class_weights(labels: Tensor, num_classes: int = 80) -> Tensor:', 'def load_class_names_from_file(path: Union[str, Path]) -> list:', 'def select_device(device: str = "") -> torch.device:', 'def scale_coords(new_image_shape, coords, raw_image_shape, ratio_pad=None) -> np.ndarray:', 'def xywh2xyxy(x: np.ndarray) -> np.ndarray:', 'def xyxy2xywh(x: np.ndarray) -> np.ndarray:', 'from .loggers import *', 'import torch.distributed as dist', 'class Summary(Enum):'], 'executed_lines': {14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 149, 26, 27, 32, 166, 49, 61, 86, 102, 118}, 'executed_function_lines': {0, 27}, 'extra_calls': 2, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'Summary', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/loggers/average_meter.py', 'lineno': 27, 'args': {}, 'lines': ['class Summary(Enum):', 'NONE = 0', 'AVERAGE = 1', 'SUM = 2', 'COUNT = 3'], 'executed_lines': {27, 28, 29, 30, 31}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .autochor import *', 'from .common import *', 'import numpy as np', 'import torch', 'from torch import Tensor', 'try:', 'import accimage', 'except ImportError:', 'accimage = None', '__all__ = [', 'def clip_coords(boxes: Tensor, image_shape: tuple) -> Tensor:', 'def coco80_to_coco91_class() -> list:', 'def labels_to_class_weights(labels: Tensor, num_classes: int = 80) -> Tensor:', 'def load_class_names_from_file(path: Union[str, Path]) -> list:', 'def select_device(device: str = "") -> torch.device:', 'def scale_coords(new_image_shape, coords, raw_image_shape, ratio_pad=None) -> np.ndarray:', 'def xywh2xyxy(x: np.ndarray) -> np.ndarray:', 'def xyxy2xywh(x: np.ndarray) -> np.ndarray:', 'from .loggers import *', 'import torch.distributed as dist', 'class Summary(Enum):', 'class AverageMeter(object):'], 'executed_lines': {14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 149, 26, 27, 32, 34, 166, 49, 61, 86, 102, 118}, 'executed_function_lines': {0, 34, 27}, 'extra_calls': 2, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'AverageMeter', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/loggers/average_meter.py', 'lineno': 34, 'args': {}, 'lines': ['class AverageMeter(object):', 'def __init__(self, name, fmt=":f", summary_type=Summary.AVERAGE):', 'def reset(self):', 'def update(self, val, n=1):', 'def all_reduce(self):', 'def __str__(self):', 'def summary(self):'], 'executed_lines': {34, 35, 71, 75, 47, 53, 59}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .autochor import *', 'from .common import *', 'import numpy as np', 'import torch', 'from torch import Tensor', 'try:', 'import accimage', 'except ImportError:', 'accimage = None', '__all__ = [', 'def clip_coords(boxes: Tensor, image_shape: tuple) -> Tensor:', 'def coco80_to_coco91_class() -> list:', 'def labels_to_class_weights(labels: Tensor, num_classes: int = 80) -> Tensor:', 'def load_class_names_from_file(path: Union[str, Path]) -> list:', 'def select_device(device: str = "") -> torch.device:', 'def scale_coords(new_image_shape, coords, raw_image_shape, ratio_pad=None) -> np.ndarray:', 'def xywh2xyxy(x: np.ndarray) -> np.ndarray:', 'def xyxy2xywh(x: np.ndarray) -> np.ndarray:', 'from .loggers import *', 'import torch.distributed as dist', 'class Summary(Enum):', 'class AverageMeter(object):'], 'executed_lines': {14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 149, 26, 27, 32, 34, 166, 49, 61, 86, 102, 118}, 'executed_function_lines': {0, 34, 27, 23}, 'extra_calls': 2, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'ProgressMeter', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/loggers/progress_meter.py', 'lineno': 23, 'args': {}, 'lines': ['class ProgressMeter(object):', 'def __init__(self, num_batches, meters, prefix=""):', 'def display(self, batch):', 'def display_summary(self):', '@staticmethod', 'def _get_batch_fmtstr(num_batches):'], 'executed_lines': {34, 39, 40, 23, 24, 29}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .autochor import *', 'from .common import *', 'import numpy as np', 'import torch', 'from torch import Tensor', 'try:', 'import accimage', 'except ImportError:', 'accimage = None', '__all__ = [', 'def clip_coords(boxes: Tensor, image_shape: tuple) -> Tensor:', 'def coco80_to_coco91_class() -> list:', 'def labels_to_class_weights(labels: Tensor, num_classes: int = 80) -> Tensor:', 'def load_class_names_from_file(path: Union[str, Path]) -> list:', 'def select_device(device: str = "") -> torch.device:', 'def scale_coords(new_image_shape, coords, raw_image_shape, ratio_pad=None) -> np.ndarray:', 'def xywh2xyxy(x: np.ndarray) -> np.ndarray:', 'def xyxy2xywh(x: np.ndarray) -> np.ndarray:', 'from .loggers import *', 'import torch.distributed as dist', 'class Summary(Enum):', 'class AverageMeter(object):', 'conf_thresh: float = 0.1,', 'iou_thresh: float = 0.6,', 'filter_classes: list = None,', 'def non_max_suppression(', 'prediction: Tensor,', ') -> Tensor:', 'line_thickness: float = None', ') -> None:', 'paths: str = None,', 'file_name: str = "images.jpg",', 'names: str = None,', 'max_size: int = 640,', 'max_subplots: int = 16,', 'def plot_images(', 'imgs: Tensor,', 'targets: Tensor,', ') -> None:'], 'executed_lines': {14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 149, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 166, 49, 61, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 102, 118}, 'executed_function_lines': {0, 34, 27, 23}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/base.py', 'lineno': 0, 'args': {}, 'lines': ['"""', 'import glob', 'import os', 'import random', 'from pathlib import Path', 'from typing import Any, Tuple, List', 'import cv2', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from PIL import ExifTags, Image', 'from skimage import io', 'from torch import Tensor', 'from torchvision.transforms import functional as F_vision', 'from tqdm import tqdm', 'from yolov3_pytorch.utils.common import xywh2xyxy, xyxy2xywh', 'from .data_augment import adjust_hsv, letterbox, random_affine', 'import numpy as np', 'def cutout(img: np.ndarray, labels: np.ndarray) -> np.ndarray:', 'new_shape: int or tuple = (416, 416),', 'color: tuple = (114, 114, 114),', 'auto: bool = True,', 'scale_fill: bool = False,', 'scaleup: bool = True', 'def letterbox(', 'img: np.ndarray,', ') -> tuple[Any, tuple[float | Any, float | Any], tuple[float | int | Any, float | int | Any]]:', 'def mixup(img1, labels, img2, labels2):', 'targets: np.ndarray = (),', 'degrees: float = 10,', 'translate: float = 0.1,', 'scale: float = 0.1,', 'shear: float = 10,', 'border: int = 0,', 'def random_affine(', 'img: np.ndarray,', ') -> tuple:', 'from .utils import IMG_FORMATS', '"BaseDatasets",', '__all__ = [', 'for k, v in ExifTags.TAGS.items():', 'if v == "Orientation":', 'ORIENTATION = k', 'break', 'for orientation in ExifTags.TAGS.keys():', 'if ExifTags.TAGS[orientation] == "Orientation":', 'break', 'class BaseDatasets(torch.utils.data.Dataset):'], 'executed_lines': {14, 143, 144, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 37, 38, 42, 43, 44, 45, 48, 49, 50, 53, 196, 70, 145, 222, 146, 224, 225, 226, 227, 147, 228, 229, 223, 230, 148, 149, 150}, 'executed_function_lines': {0, 53}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'BaseDatasets', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/base.py', 'lineno': 53, 'args': {}, 'lines': ['class BaseDatasets(torch.utils.data.Dataset):', 'img_size: int = 416,', 'batch_size: int = 16,', 'augment: bool = False,', 'augment_dict: Any = None,', 'rect_label: bool = False,', 'img_weights: bool = False,', 'cache_imgs: bool = False,', 'single_classes: bool = False,', 'pad: float = 0.0,', 'gray: bool = False,', 'def __init__(', 'path: str,', ') -> None:', '@staticmethod', 'def _exif_size(img: Image.Image) -> tuple:', 'def load_image(', 'index: int,', ') -> tuple[np.ndarray | np.ndarray[Any, np.dtype[np.generic | np.generic]] | Any, tuple[int, int], tuple[int, ...]] | tuple[None, None, None]:', 'def load_mosaic(self, index: int) -> Tuple[np.ndarray, List]:', 'def __len__(self):', 'def __getitem__(self, index: int):', '@staticmethod', 'def collate_fn(batch: Tensor) -> tuple:'], 'executed_lines': {259, 260, 283, 285, 286, 312, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 465, 466, 377, 381}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/base.py', 'lineno': 0, 'args': {}, 'lines': ['"""', 'import glob', 'import os', 'import random', 'from pathlib import Path', 'from typing import Any, Tuple, List', 'import cv2', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from PIL import ExifTags, Image', 'from skimage import io', 'from torch import Tensor', 'from torchvision.transforms import functional as F_vision', 'from tqdm import tqdm', 'from yolov3_pytorch.utils.common import xywh2xyxy, xyxy2xywh', 'from .data_augment import adjust_hsv, letterbox, random_affine', 'import numpy as np', 'def cutout(img: np.ndarray, labels: np.ndarray) -> np.ndarray:', 'new_shape: int or tuple = (416, 416),', 'color: tuple = (114, 114, 114),', 'auto: bool = True,', 'scale_fill: bool = False,', 'scaleup: bool = True', 'def letterbox(', 'img: np.ndarray,', ') -> tuple[Any, tuple[float | Any, float | Any], tuple[float | int | Any, float | int | Any]]:', 'def mixup(img1, labels, img2, labels2):', 'targets: np.ndarray = (),', 'degrees: float = 10,', 'translate: float = 0.1,', 'scale: float = 0.1,', 'shear: float = 10,', 'border: int = 0,', 'def random_affine(', 'img: np.ndarray,', ') -> tuple:', 'from .utils import IMG_FORMATS', '"BaseDatasets",', '__all__ = [', 'for k, v in ExifTags.TAGS.items():', 'if v == "Orientation":', 'ORIENTATION = k', 'break', 'for orientation in ExifTags.TAGS.keys():', 'if ExifTags.TAGS[orientation] == "Orientation":', 'break', 'class BaseDatasets(torch.utils.data.Dataset):'], 'executed_lines': {14, 143, 144, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 37, 38, 42, 43, 44, 45, 48, 49, 50, 53, 196, 70, 145, 222, 146, 224, 225, 226, 227, 147, 228, 229, 223, 230, 148, 149, 150}, 'executed_function_lines': {0, 53}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .base import *', 'from .data_augment import *', 'from .images import *', 'from typing import Union', 'import cv2', 'import numpy as np', 'import torch', 'from torchvision.transforms import functional as F_vision', 'from .data_augment import letterbox', 'from .utils import IMG_FORMATS, VID_FORMATS', '"LoadImages",', '__all__ = [', 'class LoadImages:'], 'executed_lines': {32, 14, 15, 16, 17, 19, 20, 21, 22, 24, 25, 27, 28}, 'executed_function_lines': {0, 32}, 'extra_calls': 1}

{'function_name': 'LoadImages', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/images.py', 'lineno': 32, 'args': {}, 'lines': ['class LoadImages:', 'img_size: int = 416,', 'gray: bool = False,', 'def __init__(', 'img_path: Union[str, Path],', ') -> None:', 'def new_video(self, path: str) -> None:', 'def read_video(self, path):', 'def read_image(self, path) -> np.ndarray:', 'def __iter__(self):', 'def __next__(self):', 'def __len__(self):'], 'executed_lines': {32, 33, 130, 35, 36, 37, 38, 100, 163, 117, 89, 125}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .base import *', 'from .data_augment import *', 'from .images import *', 'from typing import Union', 'import cv2', 'import numpy as np', 'import torch', 'from torchvision.transforms import functional as F_vision', 'from .data_augment import letterbox', 'from .utils import IMG_FORMATS, VID_FORMATS', '"LoadImages",', '__all__ = [', 'class LoadImages:', 'import cv2', 'from .data_augment import letterbox', '"LoadStreams",', 'class LoadStreams:'], 'executed_lines': {32, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30}, 'executed_function_lines': {0, 32, 30}, 'extra_calls': 1, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'LoadStreams', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/stream.py', 'lineno': 30, 'args': {}, 'lines': ['class LoadStreams:', 'sources="streams.txt",', 'img_size=416,', 'gray: bool = False,', 'def __init__(', ') -> None:', 'def update(self, index, cap):', 'def __iter__(self):', 'def __next__(self):', 'def __len__(self):'], 'executed_lines': {33, 34, 35, 36, 78, 110, 89, 93, 30, 31}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .base import *', 'from .data_augment import *', 'from .images import *', 'from typing import Union', 'import cv2', 'import numpy as np', 'import torch', 'from torchvision.transforms import functional as F_vision', 'from .data_augment import letterbox', 'from .utils import IMG_FORMATS, VID_FORMATS', '"LoadImages",', '__all__ = [', 'class LoadImages:', 'import cv2', 'from .data_augment import letterbox', '"LoadStreams",', 'class LoadStreams:'], 'executed_lines': {32, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30}, 'executed_function_lines': {0, 32, 27, 30}, 'extra_calls': 1, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'LoadWebcam', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/webcam.py', 'lineno': 27, 'args': {}, 'lines': ['class LoadWebcam:', 'pipe: int = 0,', 'img_size: int = 416,', 'gray: bool = False,', 'def __init__(', ') -> None:', 'def __iter__(self):', 'def __next__(self):', 'def __len__(self):'], 'executed_lines': {32, 33, 96, 51, 56, 27, 28, 30, 31}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .base import *', 'from .data_augment import *', 'from .images import *', 'from typing import Union', 'import cv2', 'import numpy as np', 'import torch', 'from torchvision.transforms import functional as F_vision', 'from .data_augment import letterbox', 'from .utils import IMG_FORMATS, VID_FORMATS', '"LoadImages",', '__all__ = [', 'class LoadImages:', 'import cv2', 'from .data_augment import letterbox', '"LoadStreams",', 'class LoadStreams:'], 'executed_lines': {32, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30}, 'executed_function_lines': {0, 32, 27, 30}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/evaler.py', 'lineno': 0, 'args': {}, 'lines': ['import json', 'from pathlib import Path', 'from typing import Dict', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from pycocotools.coco import COCO', 'from pycocotools.cocoeval import COCOeval', 'from torch import nn', 'from torchvision.ops import boxes', 'from tqdm import tqdm', 'from yolov3_pytorch.data.base import BaseDatasets', 'from yolov3_pytorch.models.darknet import Darknet', 'import numpy as np'], 'executed_lines': {14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28}, 'executed_function_lines': {0, 27}, 'extra_calls': 3}

{'function_name': 'FeatureConcat', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 27, 'args': {}, 'lines': ['class FeatureConcat(nn.Module):', 'def __init__(self, layers: nn.ModuleList) -> None:', 'def forward(self, x: Tensor) -> Tensor:'], 'executed_lines': {27, 28, 38}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'cell'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/evaler.py', 'lineno': 0, 'args': {}, 'lines': ['import json', 'from pathlib import Path', 'from typing import Dict', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from pycocotools.coco import COCO', 'from pycocotools.cocoeval import COCOeval', 'from torch import nn', 'from torchvision.ops import boxes', 'from tqdm import tqdm', 'from yolov3_pytorch.data.base import BaseDatasets', 'from yolov3_pytorch.models.darknet import Darknet', 'import numpy as np', 'class InvertedResidual(nn.Module):'], 'executed_lines': {47, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28}, 'executed_function_lines': {0, 27, 47}, 'extra_calls': 3}

{'function_name': 'InvertedResidual', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 47, 'args': {}, 'lines': ['class InvertedResidual(nn.Module):', 'def __init__(self, in_channels: int, out_channels: int, stride: int) -> None:', '@staticmethod', 'def depth_wise_conv(i, o, kernel_size, stride=1, padding=0, bias=False):', 'def forward(self, x: Tensor) -> Tensor:'], 'executed_lines': {47, 48, 83, 84, 87}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'cell'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/evaler.py', 'lineno': 0, 'args': {}, 'lines': ['import json', 'from pathlib import Path', 'from typing import Dict', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from pycocotools.coco import COCO', 'from pycocotools.cocoeval import COCOeval', 'from torch import nn', 'from torchvision.ops import boxes', 'from tqdm import tqdm', 'from yolov3_pytorch.data.base import BaseDatasets', 'from yolov3_pytorch.models.darknet import Darknet', 'import numpy as np', 'class InvertedResidual(nn.Module):', 'class MixConv2d(nn.Module):'], 'executed_lines': {99, 47, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28}, 'executed_function_lines': {0, 99, 27, 47}, 'extra_calls': 3}

{'function_name': 'MixConv2d', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 99, 'args': {}, 'lines': ['class MixConv2d(nn.Module):', 'kernel_size_tuple: tuple = (3, 5, 7),', 'stride: int = 1,', 'dilation: int = 1,', 'bias: bool = True,', 'method: str = "equal_params") -> None:', 'def __init__(', 'in_channels: int,', 'out_channels: int,', 'def forward(self, x: Tensor) -> Tensor:'], 'executed_lines': {99, 100, 102, 103, 104, 105, 106, 107, 108, 145}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'cell'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/evaler.py', 'lineno': 0, 'args': {}, 'lines': ['import json', 'from pathlib import Path', 'from typing import Dict', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from pycocotools.coco import COCO', 'from pycocotools.cocoeval import COCOeval', 'from torch import nn', 'from torchvision.ops import boxes', 'from tqdm import tqdm', 'from yolov3_pytorch.data.base import BaseDatasets', 'from yolov3_pytorch.models.darknet import Darknet', 'import numpy as np', 'class InvertedResidual(nn.Module):', 'class MixConv2d(nn.Module):', 'class WeightedFeatureFusion(nn.Module):'], 'executed_lines': {99, 47, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 150, 27, 28}, 'executed_function_lines': {0, 99, 47, 150, 27}, 'extra_calls': 3}

{'function_name': 'WeightedFeatureFusion', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 150, 'args': {}, 'lines': ['class WeightedFeatureFusion(nn.Module):', 'def __init__(self, layers: nn.ModuleList, weight: bool = False) -> None:', 'def forward(self, x: Tensor, outputs: Tensor) -> Tensor:'], 'executed_lines': {165, 150, 151}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'cell'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/evaler.py', 'lineno': 0, 'args': {}, 'lines': ['import json', 'from pathlib import Path', 'from typing import Dict', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from pycocotools.coco import COCO', 'from pycocotools.cocoeval import COCOeval', 'from torch import nn', 'from torchvision.ops import boxes', 'from tqdm import tqdm', 'from yolov3_pytorch.data.base import BaseDatasets', 'from yolov3_pytorch.models.darknet import Darknet', 'import numpy as np', 'class InvertedResidual(nn.Module):', 'class MixConv2d(nn.Module):', 'class WeightedFeatureFusion(nn.Module):', 'class YOLOLayer(nn.Module):'], 'executed_lines': {99, 197, 47, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 150, 27, 28}, 'executed_function_lines': {0, 99, 197, 47, 150, 27}, 'extra_calls': 3}

{'function_name': 'YOLOLayer', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 197, 'args': {}, 'lines': ['class YOLOLayer(nn.Module):', 'onnx_export: bool = False,', 'def __init__(', 'anchors: list,', 'num_classes: int,', 'img_size: tuple,', 'yolo_index: int,', 'layers: list,', 'stride: int,', ') -> None:', 'def create_grids(self, ng: tuple = (13, 13), device: torch.device = "cpu") -> None:', 'def forward(self, p):'], 'executed_lines': {256, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 241}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'cell'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/evaler.py', 'lineno': 0, 'args': {}, 'lines': ['import json', 'from pathlib import Path', 'from typing import Dict', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from pycocotools.coco import COCO', 'from pycocotools.cocoeval import COCOeval', 'from torch import nn', 'from torchvision.ops import boxes', 'from tqdm import tqdm', 'from yolov3_pytorch.data.base import BaseDatasets', 'from yolov3_pytorch.models.darknet import Darknet', 'import numpy as np', 'class InvertedResidual(nn.Module):', 'class MixConv2d(nn.Module):', 'class WeightedFeatureFusion(nn.Module):', 'class YOLOLayer(nn.Module):', 'def make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:', 'def fuse_conv_and_bn(conv: nn.Conv2d, bn: nn.BatchNorm2d) -> nn.Module:', 'def scale_img(img: Tensor, ratio: float = 1.0, same_shape: bool = True) -> Tensor:', 'compile_mode: bool = False,', 'def load_state_dict(', 'model: nn.Module,', 'state_dict: dict,', ') -> nn.Module:', 'def load_darknet_weights(model: nn.Module, weights_path: Union[str, Path]) -> nn.Module:', 'def save_darknet_weights(model: nn.Module, weights_path: Union[str, Path], cutoff: int = -1) -> None:', 'def profile(model: nn.Module, inputs: Tensor, device: str | torch.device = "cpu", verbose: bool = False) -> tuple[float, float, float]:', '__all__ = ['], 'executed_lines': {14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 150, 27, 28, 148, 30, 31, 32, 33, 34, 26, 296, 47, 181, 320, 197, 80, 99, 356}, 'executed_function_lines': {0, 99, 197, 47, 150, 27, 31}, 'extra_calls': 2, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'Darknet', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 31, 'args': {}, 'lines': ['class Darknet(nn.Module):', 'img_size: tuple = (416, 416),', 'gray: bool = False,', 'compile_mode: bool = False,', 'onnx_export: bool = False,', 'def __init__(', 'model_config_path: str,', ') -> None:', 'def create_module_defines(self) -> List[Dict[str, Any]]:', 'def create_module_list(self) -> [nn.ModuleList, list]:', 'def get_yolo_layers(self):', 'def fuse(self):', 'augment: bool = False', 'def forward(', 'x: Tensor,', ') -> list[Any] | tuple[Tensor, Tensor] | tuple[Tensor, Any] | tuple[Tensor, None]:', 'augment: bool = False) -> list[Any] | tuple[Tensor, Tensor] | tuple[Tensor, Any]:', 'def forward_once(', 'x: Tensor,'], 'executed_lines': {31, 32, 34, 35, 36, 37, 38, 39, 298, 301, 317, 319, 320, 321, 70, 338, 340, 341, 122}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'cell'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/evaler.py', 'lineno': 0, 'args': {}, 'lines': ['import json', 'from pathlib import Path', 'from typing import Dict', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from pycocotools.coco import COCO', 'from pycocotools.cocoeval import COCOeval', 'from torch import nn', 'from torchvision.ops import boxes', 'from tqdm import tqdm', 'from yolov3_pytorch.data.base import BaseDatasets', 'from yolov3_pytorch.models.darknet import Darknet', 'import numpy as np', 'class InvertedResidual(nn.Module):', 'class MixConv2d(nn.Module):', 'class WeightedFeatureFusion(nn.Module):', 'class YOLOLayer(nn.Module):', 'def make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:', 'def fuse_conv_and_bn(conv: nn.Conv2d, bn: nn.BatchNorm2d) -> nn.Module:', 'def scale_img(img: Tensor, ratio: float = 1.0, same_shape: bool = True) -> Tensor:', 'compile_mode: bool = False,', 'def load_state_dict(', 'model: nn.Module,', 'state_dict: dict,', ') -> nn.Module:', 'def load_darknet_weights(model: nn.Module, weights_path: Union[str, Path]) -> nn.Module:', 'def save_darknet_weights(model: nn.Module, weights_path: Union[str, Path], cutoff: int = -1) -> None:', 'def profile(model: nn.Module, inputs: Tensor, device: str | torch.device = "cpu", verbose: bool = False) -> tuple[float, float, float]:', '__all__ = [', 'def convert_model_state_dict(model_config_path: Union[str, Path], model_weights_path: Union[str, Path]) -> None:'], 'executed_lines': {14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 150, 27, 28, 148, 30, 31, 32, 33, 34, 26, 296, 47, 181, 320, 197, 80, 99, 356, 383}, 'executed_function_lines': {0, 99, 197, 47, 150, 26, 27, 31}, 'extra_calls': 3, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'BCEBlurWithLogitsLoss', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/losses/loss.py', 'lineno': 26, 'args': {}, 'lines': ['class BCEBlurWithLogitsLoss(nn.Module):', 'r"""BCEwithLogitLoss() with reduced missing label effects."""', 'def __init__(self, alpha: float = 0.05) -> None:', 'def forward(self, pred, true):'], 'executed_lines': {35, 26, 27, 29}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'cell'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/evaler.py', 'lineno': 0, 'args': {}, 'lines': ['import json', 'from pathlib import Path', 'from typing import Dict', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from pycocotools.coco import COCO', 'from pycocotools.cocoeval import COCOeval', 'from torch import nn', 'from torchvision.ops import boxes', 'from tqdm import tqdm', 'from yolov3_pytorch.data.base import BaseDatasets', 'from yolov3_pytorch.models.darknet import Darknet', 'import numpy as np', 'class InvertedResidual(nn.Module):', 'class MixConv2d(nn.Module):', 'class WeightedFeatureFusion(nn.Module):', 'class YOLOLayer(nn.Module):', 'def make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:', 'def fuse_conv_and_bn(conv: nn.Conv2d, bn: nn.BatchNorm2d) -> nn.Module:', 'def scale_img(img: Tensor, ratio: float = 1.0, same_shape: bool = True) -> Tensor:', 'compile_mode: bool = False,', 'def load_state_dict(', 'model: nn.Module,', 'state_dict: dict,', ') -> nn.Module:', 'def load_darknet_weights(model: nn.Module, weights_path: Union[str, Path]) -> nn.Module:', 'def save_darknet_weights(model: nn.Module, weights_path: Union[str, Path], cutoff: int = -1) -> None:', 'def profile(model: nn.Module, inputs: Tensor, device: str | torch.device = "cpu", verbose: bool = False) -> tuple[float, float, float]:', '__all__ = [', 'def convert_model_state_dict(model_config_path: Union[str, Path], model_weights_path: Union[str, Path]) -> None:', 'class FocalLoss(nn.Module):'], 'executed_lines': {14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 150, 27, 28, 148, 30, 31, 32, 33, 34, 26, 296, 47, 49, 181, 320, 197, 80, 99, 356, 383}, 'executed_function_lines': {0, 99, 197, 47, 49, 150, 26, 27, 31}, 'extra_calls': 3, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'FocalLoss', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/losses/loss.py', 'lineno': 49, 'args': {}, 'lines': ['class FocalLoss(nn.Module):', 'def __init__(self, loss_fcn: nn.Module, gamma: float = 1.5, alpha: float = 0.25):', 'def forward(self, pred, true):'], 'executed_lines': {49, 50, 67}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'cell'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/evaler.py', 'lineno': 0, 'args': {}, 'lines': ['import json', 'from pathlib import Path', 'from typing import Dict', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from pycocotools.coco import COCO', 'from pycocotools.cocoeval import COCOeval', 'from torch import nn', 'from torchvision.ops import boxes', 'from tqdm import tqdm', 'from yolov3_pytorch.data.base import BaseDatasets', 'from yolov3_pytorch.models.darknet import Darknet', 'import numpy as np', 'class InvertedResidual(nn.Module):', 'class MixConv2d(nn.Module):', 'class WeightedFeatureFusion(nn.Module):', 'class YOLOLayer(nn.Module):', 'def make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:', 'def fuse_conv_and_bn(conv: nn.Conv2d, bn: nn.BatchNorm2d) -> nn.Module:', 'def scale_img(img: Tensor, ratio: float = 1.0, same_shape: bool = True) -> Tensor:', 'compile_mode: bool = False,', 'def load_state_dict(', 'model: nn.Module,', 'state_dict: dict,', ') -> nn.Module:', 'def load_darknet_weights(model: nn.Module, weights_path: Union[str, Path]) -> nn.Module:', 'def save_darknet_weights(model: nn.Module, weights_path: Union[str, Path], cutoff: int = -1) -> None:', 'def profile(model: nn.Module, inputs: Tensor, device: str | torch.device = "cpu", verbose: bool = False) -> tuple[float, float, float]:', '__all__ = [', 'def convert_model_state_dict(model_config_path: Union[str, Path], model_weights_path: Union[str, Path]) -> None:', 'class FocalLoss(nn.Module):', 'iou_thresh: float = 0.5,', 'def build_targets(', 'p: Tensor,', 'targets: Tensor,', 'model: nn.Module,', ') -> tuple[list[Any], list[Tensor], list[tuple[Any, Tensor | list[Any] | Any, Any, Any]], list[Any]]:', 'def compute_loss(', 'p: Tensor,', 'targets: Tensor,', 'model: nn.Module,', 'iou_thresh: float,', 'losses_dict: Any,', 'def smooth_bce(eps: float = 0.1):', 'from yolov3_pytorch.models.utils import load_state_dict', '"Evaler",', 'class Evaler:'], 'executed_lines': {138, 139, 140, 141, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 150, 27, 28, 148, 30, 31, 32, 33, 34, 26, 29, 35, 39, 296, 47, 49, 181, 320, 197, 142, 80, 143, 212, 85, 86, 87, 88, 89, 90, 99, 356, 383}, 'executed_function_lines': {0, 99, 197, 39, 47, 49, 150, 26, 27, 31}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'Evaler', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/evaler.py', 'lineno': 39, 'args': {}, 'lines': ['class Evaler:', 'def __init__(', 'config: Dict,', 'device: torch.device,', ') -> None:', 'def load_datasets(self) -> torch.utils.data.DataLoader:', 'def build_model(self) -> nn.Module:', 'def validate(self):', '@staticmethod', 'conf_thresh: float = 0.01,', 'iou_thresh: float = 0.30,', 'iouv: tuple = (0.5, 0.95),', 'gt_json_path: str = "",', 'pred_json_path: str = "",', 'verbose: bool = False,', 'device: torch.device = torch.device("cpu"),', 'def validate_on_epoch(', 'model: nn.Module,', 'dataloader: torch.utils.data.DataLoader,', 'class_names: list,', 'augment: bool,', ') -> tuple:'], 'executed_lines': {130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 39, 40, 42, 43, 44, 48, 79, 114}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/evaler.py', 'lineno': 0, 'args': {}, 'lines': ['import json', 'from pathlib import Path', 'from typing import Dict', 'import numpy as np', 'import torch', 'import torch.utils.data', 'from pycocotools.coco import COCO', 'from pycocotools.cocoeval import COCOeval', 'from torch import nn', 'from torchvision.ops import boxes', 'from tqdm import tqdm', 'from yolov3_pytorch.data.base import BaseDatasets', 'from yolov3_pytorch.models.darknet import Darknet', 'import numpy as np', 'class InvertedResidual(nn.Module):', 'class MixConv2d(nn.Module):', 'class WeightedFeatureFusion(nn.Module):', 'class YOLOLayer(nn.Module):', 'def make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:', 'def fuse_conv_and_bn(conv: nn.Conv2d, bn: nn.BatchNorm2d) -> nn.Module:', 'def scale_img(img: Tensor, ratio: float = 1.0, same_shape: bool = True) -> Tensor:', 'compile_mode: bool = False,', 'def load_state_dict(', 'model: nn.Module,', 'state_dict: dict,', ') -> nn.Module:', 'def load_darknet_weights(model: nn.Module, weights_path: Union[str, Path]) -> nn.Module:', 'def save_darknet_weights(model: nn.Module, weights_path: Union[str, Path], cutoff: int = -1) -> None:', 'def profile(model: nn.Module, inputs: Tensor, device: str | torch.device = "cpu", verbose: bool = False) -> tuple[float, float, float]:', '__all__ = [', 'def convert_model_state_dict(model_config_path: Union[str, Path], model_weights_path: Union[str, Path]) -> None:', 'class FocalLoss(nn.Module):', 'iou_thresh: float = 0.5,', 'def build_targets(', 'p: Tensor,', 'targets: Tensor,', 'model: nn.Module,', ') -> tuple[list[Any], list[Tensor], list[tuple[Any, Tensor | list[Any] | Any, Any, Any]], list[Any]]:', 'def compute_loss(', 'p: Tensor,', 'targets: Tensor,', 'model: nn.Module,', 'iou_thresh: float,', 'losses_dict: Any,', 'def smooth_bce(eps: float = 0.1):', 'from yolov3_pytorch.models.utils import load_state_dict', '"Evaler",', 'class Evaler:'], 'executed_lines': {138, 139, 140, 141, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 150, 27, 28, 148, 30, 31, 32, 33, 34, 26, 29, 35, 39, 296, 47, 49, 181, 320, 197, 142, 80, 143, 212, 85, 86, 87, 88, 89, 90, 99, 356, 383}, 'executed_function_lines': {0, 99, 197, 39, 47, 49, 150, 26, 27, 31}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .evaler import *', 'from .inferencer import *', 'import random', 'import warnings', 'from pathlib import Path', 'import cv2', 'import torch', 'from torch import nn', 'from torch.backends import cudnn', 'from yolov3_pytorch.data import LoadImages, LoadStreams', 'from yolov3_pytorch.models import Darknet, load_state_dict', 'from yolov3_pytorch.utils import load_class_names_from_file, select_device, scale_coords, xyxy2xywh, non_max_suppression, plot_one_box', 'class Inferencer:'], 'executed_lines': {14, 15, 16, 17, 18, 20, 21, 22, 23, 25, 26, 27, 30}, 'executed_function_lines': {0, 30}, 'extra_calls': 1}

{'function_name': 'Inferencer', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 30, 'args': {}, 'lines': ['class Inferencer:', 'def __init__(self, opts: argparse.Namespace):', 'def build_model(self) -> nn.Module:', 'def inference(self) -> None:'], 'executed_lines': {76, 109, 30, 31}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .evaler import *', 'from .inferencer import *', 'import random', 'import warnings', 'from pathlib import Path', 'import cv2', 'import torch', 'from torch import nn', 'from torch.backends import cudnn', 'from yolov3_pytorch.data import LoadImages, LoadStreams', 'from yolov3_pytorch.models import Darknet, load_state_dict', 'from yolov3_pytorch.utils import load_class_names_from_file, select_device, scale_coords, xyxy2xywh, non_max_suppression, plot_one_box', 'class Inferencer:', 'from typing import Dict, Union', 'from torch import optim', 'from torch.utils.tensorboard import SummaryWriter', 'from yolov3_pytorch.engine import Evaler', 'from yolov3_pytorch.models import Darknet, load_state_dict', 'from yolov3_pytorch.models.losses import compute_loss', 'from yolov3_pytorch.utils import labels_to_class_weights, plot_images, AverageMeter, ProgressMeter', '"Trainer",', '__all__ = [', 'class Trainer:'], 'executed_lines': {14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 36, 37, 41}, 'executed_function_lines': {0, 41, 30}, 'extra_calls': 1, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'Trainer', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/trainer.py', 'lineno': 41, 'args': {}, 'lines': ['class Trainer:', 'def __init__(', 'config: Dict,', 'scaler: amp.GradScaler,', 'device: torch.device,', 'save_weights_dir: Union[str, Path],', 'tblogger: SummaryWriter,', ') -> None:', 'def load_datasets(self) -> tuple:', 'def build_model(self) -> tuple:', 'def define_optim(self) -> optim:', 'def define_lr_scheduler(self) -> optim.lr_scheduler:', 'def train_on_epoch(self, epoch):', 'def train(self):', 'def load_checkpoint(self) -> None:', 'def save_checkpoint(self, epoch: int, mean_ap: float) -> None:'], 'executed_lines': {66, 324, 41, 42, 138, 44, 45, 46, 47, 48, 49, 177, 339, 116, 280, 156}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/__init__.py', 'lineno': 0, 'args': {}, 'lines': ['from .evaler import *', 'from .inferencer import *', 'import random', 'import warnings', 'from pathlib import Path', 'import cv2', 'import torch', 'from torch import nn', 'from torch.backends import cudnn', 'from yolov3_pytorch.data import LoadImages, LoadStreams', 'from yolov3_pytorch.models import Darknet, load_state_dict', 'from yolov3_pytorch.utils import load_class_names_from_file, select_device, scale_coords, xyxy2xywh, non_max_suppression, plot_one_box', 'class Inferencer:', 'from typing import Dict, Union', 'from torch import optim', 'from torch.utils.tensorboard import SummaryWriter', 'from yolov3_pytorch.engine import Evaler', 'from yolov3_pytorch.models import Darknet, load_state_dict', 'from yolov3_pytorch.models.losses import compute_loss', 'from yolov3_pytorch.utils import labels_to_class_weights, plot_images, AverageMeter, ProgressMeter', '"Trainer",', '__all__ = [', 'class Trainer:'], 'executed_lines': {14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 36, 37, 41}, 'executed_function_lines': {0, 41, 30}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/tools/inference.py', 'lineno': 0, 'args': {}, 'lines': ['"""', 'import argparse', 'from yolov3_pytorch.engine.inferencer import Inferencer', 'def get_opts() -> argparse.Namespace:', 'def main() -> None:', 'if __name__ == "__main__":', 'main()'], 'executed_lines': {129, 136, 137, 14, 17, 19, 22}, 'executed_function_lines': {0, 129}, 'extra_calls': 0}

{'function_name': 'main', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/tools/inference.py', 'lineno': 129, 'args': {}, 'lines': ['opts = get_opts()'], 'executed_lines': {130}, 'executed_function_lines': {22}, 'extra_calls': 0}

{'function_name': 'get_opts', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/tools/inference.py', 'lineno': 22, 'args': {}, 'lines': ['parser = argparse.ArgumentParser(description="PyTorch YOLOv3 Inference")', 'parser.add_argument(', '"inputs",', 'metavar="INPUTS",', 'type=str,', 'help="path to images or video",', 'parser.add_argument(', '"--output",', 'type=str,', 'default="./results/inference/",', 'help="path to outputs dir. Default: ``./results/inference``",', 'parser.add_argument(', '"--class-names-path",', 'type=str,', 'default="./data/coco.names",', 'help="path to class names file. Default: ``./data/coco.names``",', 'parser.add_argument(', '"--model-config-path",', 'type=str,', 'default="./model_configs/COCO-Detection/yolov3.cfg",', 'help="path to model config file. Default: ``./model_configs/COCO-Detection/yolov3.cfg``",', 'parser.add_argument(', '"--img-size",', 'type=int,', 'default=416,', 'help="size of each image dimension. Default: 416",', 'parser.add_argument(', '"--gray",', 'action="store_true",', 'help="whether to grayscale image input",', 'parser.add_argument(', '"--weights",', 'type=str,', 'default="./results/pretrained_models/YOLOv3_Tiny-COCO-20231107.pth.tar",', 'help="path to weights file. Default: ``./results/pretrained_models/YOLOv3_Tiny-COCO-20231107.pth.tar``",', 'parser.add_argument(', '"--half",', 'action="store_true",', 'help="whether to half precision",', 'parser.add_argument(', '"--fuse",', 'action="store_true",', 'help="whether to fuse conv and bn",', 'parser.add_argument(', '"--show-image",', 'action="store_true",', 'help="Show image.",', 'parser.add_argument(', '"--save-txt",', 'action="store_true",', 'help="Save results to *.txt.",', 'parser.add_argument(', '"--fourcc",', 'type=str,', 'default="mp4v",', 'help="output video codec (verify ffmpeg support). Default: ``mp4v``.",', 'parser.add_argument(', '"--conf-thresh",', 'type=float,', 'default=0.25,', 'help="Object confidence threshold. Default: 0.25.")', 'parser.add_argument(', '"--iou-thresh",', 'type=float,', 'default=0.45,', 'help="IOU threshold for NMS. Default: 0.45.",', 'parser.add_argument(', '"--augment",', 'action="store_true",', 'help="Image augmented inference",', 'parser.add_argument(', '"--filter-classes",', 'nargs="+",', 'type=int,', 'help="Filter by class",', 'parser.add_argument(', '"--agnostic-nms",', 'action="store_true",', 'help="Class-agnostic NMS",', 'parser.add_argument(', '"--device",', 'default="gpu",', 'choices=["cpu", "gpu"],', 'help="Device to use. Choice: [\'cpu\', \'gpu\']. Default: ``gpu``",', 'opts = parser.parse_args()', 'return opts'], 'executed_lines': {23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 107, 108, 109, 110, 111, 113, 114, 115, 116, 118, 119, 120, 121, 122, 124, 126}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'argparse.Namespace'>", 'shape': None}}

{'function_name': 'main', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/tools/inference.py', 'lineno': 129, 'args': {}, 'lines': ['opts = get_opts()', 'app = Inferencer(opts)'], 'executed_lines': {130, 132}, 'executed_function_lines': {22, 31}, 'extra_calls': 0}

{'function_name': '__init__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 31, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}, 'opts': {'type': "<class 'argparse.Namespace'>", 'shape': None}}, 'lines': ['self.inputs = opts.inputs', 'self.output = opts.output', 'self.model_config_path = opts.model_config_path', 'self.img_size = opts.img_size', 'self.gray = opts.gray', 'self.class_names = load_class_names_from_file(opts.class_names_path)'], 'executed_lines': {32, 33, 34, 35, 36, 38}, 'executed_function_lines': {86}, 'extra_calls': 0}

{'function_name': 'load_class_names_from_file', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/common.py', 'lineno': 86, 'args': {'path': {'type': "<class 'str'>", 'shape': (17,)}}, 'lines': ['with open(path, "r") as class_names_file:', 'lines = [line.strip() for line in class_names_file.readlines()]', 'return lines'], 'executed_lines': {96, 97, 99}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'list'>", 'shape': (80,)}}

{'function_name': '__init__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 31, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}, 'opts': {'type': "<class 'argparse.Namespace'>", 'shape': None}}, 'lines': ['self.inputs = opts.inputs', 'self.output = opts.output', 'self.model_config_path = opts.model_config_path', 'self.img_size = opts.img_size', 'self.gray = opts.gray', 'self.class_names = load_class_names_from_file(opts.class_names_path)', 'self.num_classes = len(self.class_names)', 'self.colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(self.num_classes)]', 'self.weights = opts.weights', 'self.half = opts.half', 'self.fuse = opts.fuse', 'self.show_image = opts.show_image', 'self.save_txt = opts.save_txt', 'self.fourcc = opts.fourcc', 'self.conf_thresh = opts.conf_thresh', 'self.iou_thresh = opts.iou_thresh', 'self.augment = opts.augment', 'self.filter_classes = opts.filter_classes', 'self.agnostic_nms = opts.agnostic_nms', 'self.device = select_device(opts.device)'], 'executed_lines': {32, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53}, 'executed_function_lines': {102, 86}, 'extra_calls': 0}

{'function_name': 'select_device', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/common.py', 'lineno': 102, 'args': {'device': {'type': "<class 'str'>", 'shape': (3,)}}, 'lines': ['if device == "" or device == "gpu":', 'device = "cuda:0" if torch.cuda.is_available() else "cpu"', 'return torch.device(device)'], 'executed_lines': {112, 115, 111}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'torch.device'>", 'shape': None}}

{'function_name': '__init__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 31, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}, 'opts': {'type': "<class 'argparse.Namespace'>", 'shape': None}}, 'lines': ['self.inputs = opts.inputs', 'self.output = opts.output', 'self.model_config_path = opts.model_config_path', 'self.img_size = opts.img_size', 'self.gray = opts.gray', 'self.class_names = load_class_names_from_file(opts.class_names_path)', 'self.num_classes = len(self.class_names)', 'self.colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(self.num_classes)]', 'self.weights = opts.weights', 'self.half = opts.half', 'self.fuse = opts.fuse', 'self.show_image = opts.show_image', 'self.save_txt = opts.save_txt', 'self.fourcc = opts.fourcc', 'self.conf_thresh = opts.conf_thresh', 'self.iou_thresh = opts.iou_thresh', 'self.augment = opts.augment', 'self.filter_classes = opts.filter_classes', 'self.agnostic_nms = opts.agnostic_nms', 'self.device = select_device(opts.device)', 'if self.device.type == "cpu":', 'self.half = False', 'if self.inputs.startswith("rtsp") or self.inputs.startswith("http"):', 'self.detect_video = False', 'self.detect_image = True', 'self.save_image = True', 'cudnn.benchmark = False', 'self.dataset = LoadImages(self.inputs, self.img_size, self.gray)'], 'executed_lines': {32, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 59, 66, 67, 68, 69, 70}, 'executed_function_lines': {33, 102, 86}, 'extra_calls': 0}

{'function_name': '__init__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/images.py', 'lineno': 33, 'args': {'self': {'type': "<class 'yolov3_pytorch.data.images.LoadImages'>", 'shape': None}, 'img_path': {'type': "<class 'str'>", 'shape': (78,)}, 'img_size': {'type': "<class 'int'>", 'shape': None}, 'gray': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['self.img_path = img_path', 'self.img_size = img_size', 'self.gray = gray', 'self.files = []', 'self.video_flag = []', 'self.mode = "images"', 'self.frame = 0', 'self.cap = None', 'self.num_frames = 0', 'if os.path.isdir(self.img_path):', 'elif os.path.isfile(self.img_path):', 'files = [self.img_path]', 'for file in files:', 'file_extension = file.split(".")[-1].lower()', 'if file_extension in IMG_FORMATS:', 'self.files.append(file)', 'self.video_flag.append(False)', 'self.num_files = len(self.files)', 'if any(self.video_flag):', 'self.cap = None', 'assert self.num_files > 0, f"No images or videos found in {self.img_path}. " \\'], 'executed_lines': {46, 47, 48, 50, 51, 52, 53, 54, 55, 58, 60, 61, 66, 67, 68, 69, 70, 76, 78, 82, 84}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '__init__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 31, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}, 'opts': {'type': "<class 'argparse.Namespace'>", 'shape': None}}, 'lines': ['self.inputs = opts.inputs', 'self.output = opts.output', 'self.model_config_path = opts.model_config_path', 'self.img_size = opts.img_size', 'self.gray = opts.gray', 'self.class_names = load_class_names_from_file(opts.class_names_path)', 'self.num_classes = len(self.class_names)', 'self.colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(self.num_classes)]', 'self.weights = opts.weights', 'self.half = opts.half', 'self.fuse = opts.fuse', 'self.show_image = opts.show_image', 'self.save_txt = opts.save_txt', 'self.fourcc = opts.fourcc', 'self.conf_thresh = opts.conf_thresh', 'self.iou_thresh = opts.iou_thresh', 'self.augment = opts.augment', 'self.filter_classes = opts.filter_classes', 'self.agnostic_nms = opts.agnostic_nms', 'self.device = select_device(opts.device)', 'if self.device.type == "cpu":', 'self.half = False', 'if self.inputs.startswith("rtsp") or self.inputs.startswith("http"):', 'self.detect_video = False', 'self.detect_image = True', 'self.save_image = True', 'cudnn.benchmark = False', 'self.dataset = LoadImages(self.inputs, self.img_size, self.gray)', 'self.model = self.build_model()'], 'executed_lines': {32, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 59, 66, 67, 68, 69, 70, 72}, 'executed_function_lines': {33, 102, 76, 86}, 'extra_calls': 0}

{'function_name': 'build_model', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 76, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}}, 'lines': ['model = Darknet(self.model_config_path, self.img_size, self.gray)'], 'executed_lines': {78}, 'executed_function_lines': {32}, 'extra_calls': 0}

{'function_name': '__init__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 32, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}, 'model_config_path': {'type': "<class 'str'>", 'shape': (41,)}, 'img_size': {'type': "<class 'int'>", 'shape': None}, 'gray': {'type': "<class 'bool'>", 'shape': None}, 'compile_mode': {'type': "<class 'bool'>", 'shape': None}, 'onnx_export': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['super(Darknet, self).__init__()', 'self.model_config_path = model_config_path', 'self.img_size = img_size', 'self.gray = gray', 'self.compile_mode = compile_mode', 'self.onnx_export = onnx_export', 'self.module_defines = self.create_module_defines()'], 'executed_lines': {51, 52, 53, 54, 55, 56, 58}, 'executed_function_lines': {70}, 'extra_calls': 0}

{'function_name': 'create_module_defines', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 70, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}}, 'lines': ['with open(self.model_config_path, "r") as f:', 'lines = f.read().split("\\n")', 'lines = [x for x in lines if x and not x.startswith("#")]', 'lines = [x.rstrip().lstrip() for x in lines]  # get rid of fringe whitespaces', 'module_defines = []  # module definitions', 'for line in lines:', 'if line.startswith("["):  # This marks the start of a new block', 'module_defines.append({})', 'module_defines[-1]["type"] = line[1:-1].rstrip()', 'if module_defines[-1]["type"] == "convolutional":', 'key, val = line.split("=")', 'key = key.rstrip()', 'if key == "anchors":  # return nparray', 'elif (key in ["from", "layers", "mask"]) or (key == "size" and "," in val):  # return array', 'val = val.strip()', 'if val.isnumeric():  # return int or float', 'module_defines[-1][key] = int(val) if (int(val) - float(val)) == 0 else float(val)', 'module_defines[-1][key] = val  # return string', 'module_defines[-1]["batch_normalize"] = 0  # pre-populate with zeros (may be overwritten later)', 'module_defines[-1][key] = [int(x) for x in val.split(",")]', 'module_defines[-1][key] = np.array([float(x) for x in val.split(",")]).reshape((-1, 2))  # np anchors', 'supported = ["type", "in_channels", "out_channels", "in_features", "out_features",', 'f = []  # fields', 'for x in module_defines[1:]:', '[f.append(k) for k in x if k not in f]', 'u = [x for x in f if x not in supported]  # unsupported fields', 'assert not any(u), f"Unsupported fields {self.model_config_path_path}"', 'return module_defines'], 'executed_lines': {77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 92, 93, 94, 95, 97, 98, 99, 101, 104, 111, 113, 114, 116, 118, 120}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'list'>", 'shape': (108,)}}

{'function_name': '__init__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 32, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}, 'model_config_path': {'type': "<class 'str'>", 'shape': (41,)}, 'img_size': {'type': "<class 'int'>", 'shape': None}, 'gray': {'type': "<class 'bool'>", 'shape': None}, 'compile_mode': {'type': "<class 'bool'>", 'shape': None}, 'onnx_export': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['super(Darknet, self).__init__()', 'self.model_config_path = model_config_path', 'self.img_size = img_size', 'self.gray = gray', 'self.compile_mode = compile_mode', 'self.onnx_export = onnx_export', 'self.module_defines = self.create_module_defines()', 'self.module_list, self.routs = self.create_module_list()'], 'executed_lines': {51, 52, 53, 54, 55, 56, 58, 59}, 'executed_function_lines': {122, 70}, 'extra_calls': 0}

{'function_name': 'create_module_list', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 122, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}}, 'lines': ['img_size = [self.img_size] * 2 if isinstance(self.img_size, int) else self.img_size  # expand if necessary', '_ = self.module_defines.pop(0)  # cfg training hyper-params (unused)', 'output_filters = [3] if not self.gray else [1]', 'module_list = nn.ModuleList()', 'routs = []  # list of layers which rout to deeper layers', 'yolo_index = -1', 'i = 0', 'filters = 3', 'for i, module in enumerate(self.module_defines):', 'modules = nn.Sequential()', 'if module["type"] == "convolutional":', 'bn = module["batch_normalize"]', 'filters = module["filters"]', 'k = module["size"]  # kernel size', 'stride = module["stride"] if "stride" in module else (module["stride_y"], module["stride_x"])', 'if isinstance(k, int):  # single-size conv', 'modules.add_module("Conv2d", nn.Conv2d(in_channels=output_filters[-1],', 'out_channels=filters,', 'kernel_size=k,', 'stride=stride,', 'padding=k // 2 if module["pad"] else 0,', 'groups=module["groups"] if "groups" in module else 1,', 'bias=not bn))', 'if bn:', 'modules.add_module("BatchNorm2d", nn.BatchNorm2d(filters, momentum=0.03, eps=1E-4))', 'if module["activation"] == "leaky":', 'modules.add_module("activation", nn.LeakyReLU(0.1, True))', 'module_list.append(modules)', 'output_filters.append(filters)', 'elif module["type"] == "BatchNorm2d":', 'elif module["type"] == "maxpool":', 'elif module["type"] == "avgpool":', 'elif module["type"] == "squeeze_excitation":', 'elif module["type"] == "InvertedResidual":', 'elif module["type"] == "dense":', 'elif module["type"] == "upsample":', 'elif module["type"] == "route":  # nn.Sequential() placeholder for "route" layer', 'elif module["type"] == "shortcut":  # nn.Sequential() placeholder for "shortcut" layer', 'layers = module["from"]', 'filters = output_filters[-1]', 'routs.extend([i + layer if layer < 0 else layer for layer in layers])', 'modules = WeightedFeatureFusion(layers=layers, weight="weights_type" in module)'], 'executed_lines': {130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 287, 288, 162, 163, 167, 168, 180, 187, 197, 203, 210, 218, 231, 238, 244, 245, 246, 247, 248}, 'executed_function_lines': {151}, 'extra_calls': 0}

{'function_name': '__init__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 151, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.module.WeightedFeatureFusion'>", 'shape': None}, 'layers': {'type': "<class 'list'>", 'shape': (1,)}, 'weight': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['super(WeightedFeatureFusion, self).__init__()', 'self.layers = layers  # layer indices', 'self.weight = weight  # apply weights boolean', 'self.n = len(layers) + 1  # number of layers', 'if weight:'], 'executed_lines': {160, 161, 162, 158, 159}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'create_module_list', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 122, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}}, 'lines': ['img_size = [self.img_size] * 2 if isinstance(self.img_size, int) else self.img_size  # expand if necessary', '_ = self.module_defines.pop(0)  # cfg training hyper-params (unused)', 'output_filters = [3] if not self.gray else [1]', 'module_list = nn.ModuleList()', 'routs = []  # list of layers which rout to deeper layers', 'yolo_index = -1', 'i = 0', 'filters = 3', 'for i, module in enumerate(self.module_defines):', 'modules = nn.Sequential()', 'if module["type"] == "convolutional":', 'bn = module["batch_normalize"]', 'filters = module["filters"]', 'k = module["size"]  # kernel size', 'stride = module["stride"] if "stride" in module else (module["stride_y"], module["stride_x"])', 'if isinstance(k, int):  # single-size conv', 'modules.add_module("Conv2d", nn.Conv2d(in_channels=output_filters[-1],', 'out_channels=filters,', 'kernel_size=k,', 'stride=stride,', 'padding=k // 2 if module["pad"] else 0,', 'groups=module["groups"] if "groups" in module else 1,', 'bias=not bn))', 'if bn:', 'modules.add_module("BatchNorm2d", nn.BatchNorm2d(filters, momentum=0.03, eps=1E-4))', 'if module["activation"] == "leaky":', 'modules.add_module("activation", nn.LeakyReLU(0.1, True))', 'module_list.append(modules)', 'output_filters.append(filters)', 'elif module["type"] == "BatchNorm2d":', 'elif module["type"] == "maxpool":', 'elif module["type"] == "avgpool":', 'elif module["type"] == "squeeze_excitation":', 'elif module["type"] == "InvertedResidual":', 'elif module["type"] == "dense":', 'elif module["type"] == "upsample":', 'elif module["type"] == "route":  # nn.Sequential() placeholder for "route" layer', 'elif module["type"] == "shortcut":  # nn.Sequential() placeholder for "shortcut" layer', 'layers = module["from"]', 'filters = output_filters[-1]', 'routs.extend([i + layer if layer < 0 else layer for layer in layers])', 'modules = WeightedFeatureFusion(layers=layers, weight="weights_type" in module)', 'super(WeightedFeatureFusion, self).__init__()', 'self.layers = layers  # layer indices', 'self.weight = weight  # apply weights boolean', 'self.n = len(layers) + 1  # number of layers', 'routs.append(i)  # detection output (goes into yolo layer)', 'elif module["activation"] == "relu":', 'elif module["activation"] == "relu6":', 'elif module["activation"] == "mish":', 'elif module["activation"] == "hard_swish":', 'elif module["activation"] == "hard_sigmoid":', 'elif module["type"] == "reorg3d":  # yolov3_pytorch-spp-pan-scale', 'elif module["type"] == "yolo":', 'yolo_index += 1', 'stride = [32, 16, 8]  # P5, P4, P3 strides', 'if any(x in self.model_config_path for x in ["panet", "yolov4", "cd53"]):  # stride order reversed'], 'executed_lines': {256, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 158, 287, 288, 159, 162, 163, 160, 161, 165, 167, 168, 169, 171, 173, 175, 177, 180, 187, 197, 203, 210, 218, 231, 238, 244, 245, 246, 247, 248, 250, 253, 254, 255}, 'executed_function_lines': {256, 151}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<genexpr>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 256, 'args': {'.0': {'type': "<class 'tuple_iterator'>", 'shape': None}}, 'lines': ['if any(x in self.model_config_path for x in ["panet", "yolov4", "cd53"]):  # stride order reversed'], 'executed_lines': {256}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'bool'>", 'shape': None}}

{'function_name': 'create_module_list', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 122, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}}, 'lines': ['img_size = [self.img_size] * 2 if isinstance(self.img_size, int) else self.img_size  # expand if necessary', '_ = self.module_defines.pop(0)  # cfg training hyper-params (unused)', 'output_filters = [3] if not self.gray else [1]', 'module_list = nn.ModuleList()', 'routs = []  # list of layers which rout to deeper layers', 'yolo_index = -1', 'i = 0', 'filters = 3', 'for i, module in enumerate(self.module_defines):', 'modules = nn.Sequential()', 'if module["type"] == "convolutional":', 'bn = module["batch_normalize"]', 'filters = module["filters"]', 'k = module["size"]  # kernel size', 'stride = module["stride"] if "stride" in module else (module["stride_y"], module["stride_x"])', 'if isinstance(k, int):  # single-size conv', 'modules.add_module("Conv2d", nn.Conv2d(in_channels=output_filters[-1],', 'out_channels=filters,', 'kernel_size=k,', 'stride=stride,', 'padding=k // 2 if module["pad"] else 0,', 'groups=module["groups"] if "groups" in module else 1,', 'bias=not bn))', 'if bn:', 'modules.add_module("BatchNorm2d", nn.BatchNorm2d(filters, momentum=0.03, eps=1E-4))', 'if module["activation"] == "leaky":', 'modules.add_module("activation", nn.LeakyReLU(0.1, True))', 'module_list.append(modules)', 'output_filters.append(filters)', 'elif module["type"] == "BatchNorm2d":', 'elif module["type"] == "maxpool":', 'elif module["type"] == "avgpool":', 'elif module["type"] == "squeeze_excitation":', 'elif module["type"] == "InvertedResidual":', 'elif module["type"] == "dense":', 'elif module["type"] == "upsample":', 'elif module["type"] == "route":  # nn.Sequential() placeholder for "route" layer', 'elif module["type"] == "shortcut":  # nn.Sequential() placeholder for "shortcut" layer', 'layers = module["from"]', 'filters = output_filters[-1]', 'routs.extend([i + layer if layer < 0 else layer for layer in layers])', 'modules = WeightedFeatureFusion(layers=layers, weight="weights_type" in module)', 'super(WeightedFeatureFusion, self).__init__()', 'self.layers = layers  # layer indices', 'self.weight = weight  # apply weights boolean', 'self.n = len(layers) + 1  # number of layers', 'routs.append(i)  # detection output (goes into yolo layer)', 'elif module["activation"] == "relu":', 'elif module["activation"] == "relu6":', 'elif module["activation"] == "mish":', 'elif module["activation"] == "hard_swish":', 'elif module["activation"] == "hard_sigmoid":', 'elif module["type"] == "reorg3d":  # yolov3_pytorch-spp-pan-scale', 'elif module["type"] == "yolo":', 'yolo_index += 1', 'stride = [32, 16, 8]  # P5, P4, P3 strides', 'if any(x in self.model_config_path for x in ["panet", "yolov4", "cd53"]):  # stride order reversed', 'layers = module["from"] if "from" in module else []', 'modules = YOLOLayer(anchors=module["anchors"][module["mask"]],  # anchor list', 'num_classes=module["classes"],  # number of classes', 'img_size=img_size,  # (416, 416)', 'yolo_index=yolo_index,  # 0, 1, 2...', 'layers=layers,  # output layers', 'stride=stride[yolo_index])'], 'executed_lines': {256, 130, 131, 132, 133, 134, 135, 136, 137, 258, 139, 140, 260, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 259, 158, 287, 288, 159, 162, 163, 160, 161, 165, 167, 168, 169, 261, 171, 262, 173, 263, 175, 177, 264, 180, 187, 197, 203, 210, 218, 231, 238, 244, 245, 246, 247, 248, 250, 253, 254, 255}, 'executed_function_lines': {256, 198, 151}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '__init__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 198, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.module.YOLOLayer'>", 'shape': None}, 'anchors': {'type': "<class 'numpy.ndarray'>", 'shape': (3, 2)}, 'num_classes': {'type': "<class 'int'>", 'shape': None}, 'img_size': {'type': "<class 'list'>", 'shape': (2,)}, 'yolo_index': {'type': "<class 'int'>", 'shape': None}, 'layers': {'type': "<class 'list'>", 'shape': (0,)}, 'stride': {'type': "<class 'int'>", 'shape': None}, 'onnx_export': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['super(YOLOLayer, self).__init__()', 'self.anchors = torch.Tensor(anchors)', 'self.num_classes = num_classes', 'self.img_size = img_size', 'self.yolo_index = yolo_index  # index of this layer in layers', 'self.layers = layers  # model output layer indices', 'self.stride = stride  # layer stride', 'self.onnx_export = onnx_export', 'self.nl = len(layers)  # number of output layers (3)', 'self.na = len(anchors)  # number of anchors (3)', 'self.num_classes = num_classes  # number of classes (80)', 'self.num_classes_output = num_classes + 5  # number of outputs (85)', 'self.nx, self.ny, self.ng = 0, 0, 0  # initialize number of x, y grid points', 'self.anchor_vec = self.anchors / self.stride', 'self.anchor_wh = self.anchor_vec.view(1, self.na, 1, 1, 2)', 'self.grid = None', 'if self.onnx_export:'], 'executed_lines': {224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 237, 220, 221, 222, 223}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'create_module_list', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 122, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}}, 'lines': ['img_size = [self.img_size] * 2 if isinstance(self.img_size, int) else self.img_size  # expand if necessary', '_ = self.module_defines.pop(0)  # cfg training hyper-params (unused)', 'output_filters = [3] if not self.gray else [1]', 'module_list = nn.ModuleList()', 'routs = []  # list of layers which rout to deeper layers', 'yolo_index = -1', 'i = 0', 'filters = 3', 'for i, module in enumerate(self.module_defines):', 'modules = nn.Sequential()', 'if module["type"] == "convolutional":', 'bn = module["batch_normalize"]', 'filters = module["filters"]', 'k = module["size"]  # kernel size', 'stride = module["stride"] if "stride" in module else (module["stride_y"], module["stride_x"])', 'if isinstance(k, int):  # single-size conv', 'modules.add_module("Conv2d", nn.Conv2d(in_channels=output_filters[-1],', 'out_channels=filters,', 'kernel_size=k,', 'stride=stride,', 'padding=k // 2 if module["pad"] else 0,', 'groups=module["groups"] if "groups" in module else 1,', 'bias=not bn))', 'if bn:', 'modules.add_module("BatchNorm2d", nn.BatchNorm2d(filters, momentum=0.03, eps=1E-4))', 'if module["activation"] == "leaky":', 'modules.add_module("activation", nn.LeakyReLU(0.1, True))', 'module_list.append(modules)', 'output_filters.append(filters)', 'elif module["type"] == "BatchNorm2d":', 'elif module["type"] == "maxpool":', 'elif module["type"] == "avgpool":', 'elif module["type"] == "squeeze_excitation":', 'elif module["type"] == "InvertedResidual":', 'elif module["type"] == "dense":', 'elif module["type"] == "upsample":', 'elif module["type"] == "route":  # nn.Sequential() placeholder for "route" layer', 'elif module["type"] == "shortcut":  # nn.Sequential() placeholder for "shortcut" layer', 'layers = module["from"]', 'filters = output_filters[-1]', 'routs.extend([i + layer if layer < 0 else layer for layer in layers])', 'modules = WeightedFeatureFusion(layers=layers, weight="weights_type" in module)', 'super(WeightedFeatureFusion, self).__init__()', 'self.layers = layers  # layer indices', 'self.weight = weight  # apply weights boolean', 'self.n = len(layers) + 1  # number of layers', 'routs.append(i)  # detection output (goes into yolo layer)', 'elif module["activation"] == "relu":', 'elif module["activation"] == "relu6":', 'elif module["activation"] == "mish":', 'elif module["activation"] == "hard_swish":', 'elif module["activation"] == "hard_sigmoid":', 'elif module["type"] == "reorg3d":  # yolov3_pytorch-spp-pan-scale', 'elif module["type"] == "yolo":', 'yolo_index += 1', 'stride = [32, 16, 8]  # P5, P4, P3 strides', 'if any(x in self.model_config_path for x in ["panet", "yolov4", "cd53"]):  # stride order reversed', 'layers = module["from"] if "from" in module else []', 'modules = YOLOLayer(anchors=module["anchors"][module["mask"]],  # anchor list', 'num_classes=module["classes"],  # number of classes', 'img_size=img_size,  # (416, 416)', 'yolo_index=yolo_index,  # 0, 1, 2...', 'layers=layers,  # output layers', 'stride=stride[yolo_index])', 'try:', 'j = layers[yolo_index] if "from" in module else -1', 'if self.module_defines[j].__class__.__name__ == "Dropout":', 'bias_ = self.module_defines[j][0].bias  # shape(255,)', 'except:', 'pass', 'layers = module["layers"]', 'filters = sum([output_filters[layer + 1 if layer > 0 else layer] for layer in layers])', 'routs.extend([i + layer if layer < 0 else layer for layer in layers])', 'modules = FeatureConcat(layers=layers)'], 'executed_lines': {256, 130, 131, 132, 133, 134, 135, 136, 137, 258, 139, 140, 260, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 259, 277, 278, 158, 287, 288, 159, 162, 163, 160, 161, 165, 167, 168, 169, 261, 171, 262, 173, 263, 175, 177, 264, 180, 187, 267, 197, 268, 203, 270, 210, 272, 218, 231, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 250, 253, 254, 255}, 'executed_function_lines': {256, 28, 198, 151}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '__init__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 28, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.module.FeatureConcat'>", 'shape': None}, 'layers': {'type': "<class 'list'>", 'shape': (1,)}}, 'lines': ['super(FeatureConcat, self).__init__()', 'self.layers = layers', 'self.multiple = len(layers) > 1'], 'executed_lines': {34, 35, 36}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'create_module_list', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 122, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}}, 'lines': ['img_size = [self.img_size] * 2 if isinstance(self.img_size, int) else self.img_size  # expand if necessary', '_ = self.module_defines.pop(0)  # cfg training hyper-params (unused)', 'output_filters = [3] if not self.gray else [1]', 'module_list = nn.ModuleList()', 'routs = []  # list of layers which rout to deeper layers', 'yolo_index = -1', 'i = 0', 'filters = 3', 'for i, module in enumerate(self.module_defines):', 'modules = nn.Sequential()', 'if module["type"] == "convolutional":', 'bn = module["batch_normalize"]', 'filters = module["filters"]', 'k = module["size"]  # kernel size', 'stride = module["stride"] if "stride" in module else (module["stride_y"], module["stride_x"])', 'if isinstance(k, int):  # single-size conv', 'modules.add_module("Conv2d", nn.Conv2d(in_channels=output_filters[-1],', 'out_channels=filters,', 'kernel_size=k,', 'stride=stride,', 'padding=k // 2 if module["pad"] else 0,', 'groups=module["groups"] if "groups" in module else 1,', 'bias=not bn))', 'if bn:', 'modules.add_module("BatchNorm2d", nn.BatchNorm2d(filters, momentum=0.03, eps=1E-4))', 'if module["activation"] == "leaky":', 'modules.add_module("activation", nn.LeakyReLU(0.1, True))', 'module_list.append(modules)', 'output_filters.append(filters)', 'elif module["type"] == "BatchNorm2d":', 'elif module["type"] == "maxpool":', 'elif module["type"] == "avgpool":', 'elif module["type"] == "squeeze_excitation":', 'elif module["type"] == "InvertedResidual":', 'elif module["type"] == "dense":', 'elif module["type"] == "upsample":', 'elif module["type"] == "route":  # nn.Sequential() placeholder for "route" layer', 'elif module["type"] == "shortcut":  # nn.Sequential() placeholder for "shortcut" layer', 'layers = module["from"]', 'filters = output_filters[-1]', 'routs.extend([i + layer if layer < 0 else layer for layer in layers])', 'modules = WeightedFeatureFusion(layers=layers, weight="weights_type" in module)', 'super(WeightedFeatureFusion, self).__init__()', 'self.layers = layers  # layer indices', 'self.weight = weight  # apply weights boolean', 'self.n = len(layers) + 1  # number of layers', 'routs.append(i)  # detection output (goes into yolo layer)', 'elif module["activation"] == "relu":', 'elif module["activation"] == "relu6":', 'elif module["activation"] == "mish":', 'elif module["activation"] == "hard_swish":', 'elif module["activation"] == "hard_sigmoid":', 'elif module["type"] == "reorg3d":  # yolov3_pytorch-spp-pan-scale', 'elif module["type"] == "yolo":', 'yolo_index += 1', 'stride = [32, 16, 8]  # P5, P4, P3 strides', 'if any(x in self.model_config_path for x in ["panet", "yolov4", "cd53"]):  # stride order reversed', 'layers = module["from"] if "from" in module else []', 'modules = YOLOLayer(anchors=module["anchors"][module["mask"]],  # anchor list', 'num_classes=module["classes"],  # number of classes', 'img_size=img_size,  # (416, 416)', 'yolo_index=yolo_index,  # 0, 1, 2...', 'layers=layers,  # output layers', 'stride=stride[yolo_index])', 'try:', 'j = layers[yolo_index] if "from" in module else -1', 'if self.module_defines[j].__class__.__name__ == "Dropout":', 'bias_ = self.module_defines[j][0].bias  # shape(255,)', 'except:', 'pass', 'layers = module["layers"]', 'filters = sum([output_filters[layer + 1 if layer > 0 else layer] for layer in layers])', 'routs.extend([i + layer if layer < 0 else layer for layer in layers])', 'modules = FeatureConcat(layers=layers)', 'if self.onnx_export:  # explicitly state size, avoid scale_factor', 'modules = nn.Upsample(scale_factor=module["stride"])', 'super(FeatureConcat, self).__init__()', 'self.layers = layers', 'self.multiple = len(layers) > 1', 'super(YOLOLayer, self).__init__()', 'self.anchors = torch.Tensor(anchors)', 'self.num_classes = num_classes', 'self.img_size = img_size', 'self.yolo_index = yolo_index  # index of this layer in layers', 'self.layers = layers  # model output layer indices', 'self.stride = stride  # layer stride', 'self.onnx_export = onnx_export', 'self.nl = len(layers)  # number of output layers (3)', 'self.na = len(anchors)  # number of anchors (3)', 'self.num_classes = num_classes  # number of classes (80)', 'self.anchor_vec = self.anchors / self.stride', 'self.anchor_wh = self.anchor_vec.view(1, self.na, 1, 1, 2)', 'self.grid = None', 'if self.onnx_export:', 'routs_binary = [False] * (i + 1)', 'for i in routs:', 'routs_binary[i] = True', 'return module_list, routs_binary'], 'executed_lines': {34, 35, 36, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 158, 159, 160, 161, 162, 163, 165, 167, 168, 169, 171, 173, 175, 177, 180, 187, 197, 203, 210, 218, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 250, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 267, 268, 270, 272, 277, 278, 287, 288, 290, 293, 294, 296}, 'executed_function_lines': {256, 28, 198, 151}, 'extra_calls': 0, 'return_value': {'type': "<class 'tuple'>", 'shape': (2,)}}

{'function_name': '__init__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 32, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}, 'model_config_path': {'type': "<class 'str'>", 'shape': (41,)}, 'img_size': {'type': "<class 'int'>", 'shape': None}, 'gray': {'type': "<class 'bool'>", 'shape': None}, 'compile_mode': {'type': "<class 'bool'>", 'shape': None}, 'onnx_export': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['super(Darknet, self).__init__()', 'self.model_config_path = model_config_path', 'self.img_size = img_size', 'self.gray = gray', 'self.compile_mode = compile_mode', 'self.onnx_export = onnx_export', 'self.module_defines = self.create_module_defines()', 'self.module_list, self.routs = self.create_module_list()', 'self.yolo_layers = self.get_yolo_layers()'], 'executed_lines': {51, 52, 53, 54, 55, 56, 58, 59, 60}, 'executed_function_lines': {122, 298, 70}, 'extra_calls': 0}

{'function_name': 'get_yolo_layers', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 298, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}}, 'lines': ['return [i for i, m in enumerate(self.module_list) if m.__class__.__name__ == "YOLOLayer"]'], 'executed_lines': {299}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'list'>", 'shape': (3,)}}

{'function_name': '__init__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 32, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}, 'model_config_path': {'type': "<class 'str'>", 'shape': (41,)}, 'img_size': {'type': "<class 'int'>", 'shape': None}, 'gray': {'type': "<class 'bool'>", 'shape': None}, 'compile_mode': {'type': "<class 'bool'>", 'shape': None}, 'onnx_export': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['super(Darknet, self).__init__()', 'self.model_config_path = model_config_path', 'self.img_size = img_size', 'self.gray = gray', 'self.compile_mode = compile_mode', 'self.onnx_export = onnx_export', 'self.module_defines = self.create_module_defines()', 'self.module_list, self.routs = self.create_module_list()', 'self.yolo_layers = self.get_yolo_layers()', 'self.giou_ratio = 1.0', 'self.version = np.array([0, 1, 5], dtype=np.int32)  # (int32) version info: major, minor, revision', 'self.seen = 0', 'self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)'], 'executed_lines': {66, 67, 68, 51, 52, 53, 54, 55, 56, 58, 59, 60, 63}, 'executed_function_lines': {122, 298, 70}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'build_model', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 76, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}}, 'lines': ['model = Darknet(self.model_config_path, self.img_size, self.gray)', 'model = model.to(self.device)', 'model.num_classes = self.num_classes', 'with torch.no_grad():', 'if self.weights.endswith(".pth.tar"):', 'ckpt = torch.load(self.weights, map_location=self.device)', 'state_dict = ckpt.get("state_dict")', 'ema_state_dict = ckpt.get("ema_state_dict")', 'if state_dict:', 'model = load_state_dict(model, state_dict, False)'], 'executed_lines': {78, 79, 81, 84, 85, 86, 87, 88, 89, 90}, 'executed_function_lines': {32, 30}, 'extra_calls': 0}

{'function_name': 'load_state_dict', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/utils.py', 'lineno': 30, 'args': {'model': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}, 'state_dict': {'type': "<class 'collections.OrderedDict'>", 'shape': (70,)}, 'compile_mode': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['if int(torch.__version__[0]) < 2 and compile_mode:', 'compile_keyword = "_orig_mod"', 'model_state_dict = model.state_dict()', 'new_state_dict = OrderedDict()', 'for k, v in state_dict.items():', 'k_prefix = k.split(".")[0]', 'if k_prefix == compile_keyword and not compile_mode:', 'elif k_prefix != compile_keyword and compile_mode:', 'name = k', 'new_state_dict[name] = v', 'state_dict = new_state_dict', 'new_state_dict = {k: v for k, v in state_dict.items() if k in model_state_dict.keys() and v.size() == model_state_dict[k].size()}', 'model_state_dict.update(new_state_dict)', 'model.load_state_dict(model_state_dict)', 'return model'], 'executed_lines': {66, 67, 68, 71, 74, 75, 77, 46, 51, 54, 55, 58, 59, 61, 63}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}}

{'function_name': 'build_model', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 76, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}}, 'lines': ['model = Darknet(self.model_config_path, self.img_size, self.gray)', 'model = model.to(self.device)', 'model.num_classes = self.num_classes', 'with torch.no_grad():', 'if self.weights.endswith(".pth.tar"):', 'ckpt = torch.load(self.weights, map_location=self.device)', 'state_dict = ckpt.get("state_dict")', 'ema_state_dict = ckpt.get("ema_state_dict")', 'if state_dict:', 'model = load_state_dict(model, state_dict, False)', 'print(f"Loaded `{self.weights}` models weights successfully.")', 'if self.half:', 'if self.fuse:', 'return model'], 'executed_lines': {99, 101, 104, 107, 78, 79, 81, 84, 85, 86, 87, 88, 89, 90}, 'executed_function_lines': {32, 30}, 'extra_calls': 0, 'return_value': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}}

{'function_name': '__init__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 31, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}, 'opts': {'type': "<class 'argparse.Namespace'>", 'shape': None}}, 'lines': ['self.inputs = opts.inputs', 'self.output = opts.output', 'self.model_config_path = opts.model_config_path', 'self.img_size = opts.img_size', 'self.gray = opts.gray', 'self.class_names = load_class_names_from_file(opts.class_names_path)', 'self.num_classes = len(self.class_names)', 'self.colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(self.num_classes)]', 'self.weights = opts.weights', 'self.half = opts.half', 'self.fuse = opts.fuse', 'self.show_image = opts.show_image', 'self.save_txt = opts.save_txt', 'self.fourcc = opts.fourcc', 'self.conf_thresh = opts.conf_thresh', 'self.iou_thresh = opts.iou_thresh', 'self.augment = opts.augment', 'self.filter_classes = opts.filter_classes', 'self.agnostic_nms = opts.agnostic_nms', 'self.device = select_device(opts.device)', 'if self.device.type == "cpu":', 'self.half = False', 'if self.inputs.startswith("rtsp") or self.inputs.startswith("http"):', 'self.detect_video = False', 'self.detect_image = True', 'self.save_image = True', 'cudnn.benchmark = False', 'self.dataset = LoadImages(self.inputs, self.img_size, self.gray)', 'self.model = self.build_model()', 'os.makedirs(self.output, exist_ok=True)'], 'executed_lines': {32, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 59, 66, 67, 68, 69, 70, 72, 74}, 'executed_function_lines': {33, 102, 76, 86}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'main', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/tools/inference.py', 'lineno': 129, 'args': {}, 'lines': ['opts = get_opts()', 'app = Inferencer(opts)', 'app.inference()'], 'executed_lines': {130, 132, 133}, 'executed_function_lines': {109, 22, 31}, 'extra_calls': 0}

{'function_name': 'inference', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 109, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}}, 'lines': ['print("here")', 'self.model.eval()', 'for path, img, raw_img, video_capture in self.dataset:'], 'executed_lines': {113, 110, 111}, 'executed_function_lines': {125}, 'extra_calls': 0}

{'function_name': '__iter__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/images.py', 'lineno': 125, 'args': {'self': {'type': "<class 'yolov3_pytorch.data.images.LoadImages'>", 'shape': (1,)}}, 'lines': ['self.count = 0', 'return self'], 'executed_lines': {128, 127}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'yolov3_pytorch.data.images.LoadImages'>", 'shape': (1,)}}

{'function_name': 'inference', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 109, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}}, 'lines': ['print("here")', 'self.model.eval()', 'for path, img, raw_img, video_capture in self.dataset:'], 'executed_lines': {113, 110, 111}, 'executed_function_lines': {130, 125}, 'extra_calls': 0}

{'function_name': '__next__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/images.py', 'lineno': 130, 'args': {'self': {'type': "<class 'yolov3_pytorch.data.images.LoadImages'>", 'shape': (1,)}}, 'lines': ['if self.count == self.num_files:', 'path = self.files[self.count]', 'video_flag = self.video_flag[self.count]', 'if video_flag:', 'raw_img = self.read_image(path)'], 'executed_lines': {133, 136, 137, 139, 145}, 'executed_function_lines': {117}, 'extra_calls': 0}

{'function_name': 'read_image', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/images.py', 'lineno': 117, 'args': {'self': {'type': "<class 'yolov3_pytorch.data.images.LoadImages'>", 'shape': (1,)}, 'path': {'type': "<class 'str'>", 'shape': (78,)}}, 'lines': ['self.count += 1', 'raw_img = cv2.imread(path)', 'assert raw_img is not None, "Image Not Found " + path', 'print(f"image {self.count}/{self.num_files} {path}: ", end="")', 'return raw_img'], 'executed_lines': {118, 119, 120, 121, 123}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'numpy.ndarray'>", 'shape': (576, 768, 3)}}

{'function_name': '__next__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/images.py', 'lineno': 130, 'args': {'self': {'type': "<class 'yolov3_pytorch.data.images.LoadImages'>", 'shape': (1,)}}, 'lines': ['if self.count == self.num_files:', 'path = self.files[self.count]', 'video_flag = self.video_flag[self.count]', 'if video_flag:', 'raw_img = self.read_image(path)', 'img = letterbox(raw_img, new_shape=self.img_size)[0]'], 'executed_lines': {133, 136, 137, 139, 145, 148}, 'executed_function_lines': {117, 143}, 'extra_calls': 0}

{'function_name': 'letterbox', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/data_augment.py', 'lineno': 143, 'args': {'img': {'type': "<class 'numpy.ndarray'>", 'shape': (576, 768, 3)}, 'new_shape': {'type': "<class 'int'>", 'shape': None}, 'color': {'type': "<class 'tuple'>", 'shape': (3,)}, 'auto': {'type': "<class 'bool'>", 'shape': None}, 'scale_fill': {'type': "<class 'bool'>", 'shape': None}, 'scaleup': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['shape = img.shape[:2]  # current shape [height, width]', 'if isinstance(new_shape, int):', 'new_shape = (new_shape, new_shape)', 'r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])', 'if not scaleup:  # only scale down, do not scale up (for better test mAP)', 'ratio = r, r  # width, height ratios', 'new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))', 'dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding', 'if auto:  # minimum rectangle', 'dw, dh = np.mod(dw, 32), np.mod(dh, 32)  # wh padding', 'dw /= 2  # divide padding into 2 sides', 'dh /= 2', 'if shape[::-1] != new_unpad:  # resize', 'img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)', 'top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))', 'left, right = int(round(dw - 0.1)), int(round(dw + 0.1))', 'img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border', 'return img, ratio, (dw, dh)'], 'executed_lines': {192, 193, 165, 166, 167, 170, 171, 175, 176, 177, 178, 179, 185, 186, 188, 189, 190, 191}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'tuple'>", 'shape': (3,)}}

{'function_name': '__next__', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/data/images.py', 'lineno': 130, 'args': {'self': {'type': "<class 'yolov3_pytorch.data.images.LoadImages'>", 'shape': (1,)}}, 'lines': ['if self.count == self.num_files:', 'path = self.files[self.count]', 'video_flag = self.video_flag[self.count]', 'if video_flag:', 'raw_img = self.read_image(path)', 'img = letterbox(raw_img, new_shape=self.img_size)[0]', 'img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416', 'img = np.ascontiguousarray(img)', 'img = torch.from_numpy(img)', 'if self.gray:', 'return path, img, raw_img, self.cap'], 'executed_lines': {161, 133, 136, 137, 139, 145, 148, 151, 152, 155, 157}, 'executed_function_lines': {117, 143}, 'extra_calls': 0, 'return_value': {'type': "<class 'tuple'>", 'shape': (4,)}}

{'function_name': 'inference', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 109, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}}, 'lines': ['print("here")', 'self.model.eval()', 'for path, img, raw_img, video_capture in self.dataset:', 'img = img.to(self.device)', 'img = img.half() if self.half else img.float()', 'img /= 255.0', 'if img.ndimension() == 3:', 'img = img.unsqueeze(0)', 'with torch.no_grad():', 'output, _ = self.model(img, self.augment)'], 'executed_lines': {110, 111, 113, 115, 116, 117, 119, 120, 123, 124}, 'executed_function_lines': {130, 317, 125}, 'extra_calls': 0}

{'function_name': 'forward', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 317, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}, 'x': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 3, 320, 416])}, 'augment': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['if not augment:', 'return self.forward_once(x)'], 'executed_lines': {322, 323}, 'executed_function_lines': {338}, 'extra_calls': 0}

{'function_name': 'forward_once', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 338, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}, 'x': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 3, 320, 416])}, 'augment': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['img_size = x.shape[-2:]  # height, width', 'yolo_out, out = [], []', 'batch_size = x.shape[0]', 'scale_factor = [0.83, 0.67]', 'if augment:', 'for i, module in enumerate(self.module_list):', 'name = module.__class__.__name__', 'if name == "WeightedFeatureFusion":', 'elif name == "FeatureConcat":', 'elif name == "YOLOLayer":', 'x = module(x)', 'out.append(x if self.routs[i] else [])', 'x = module(x, out)'], 'executed_lines': {353, 354, 355, 356, 357, 359, 362, 364, 342, 343, 346, 347, 350}, 'executed_function_lines': {165}, 'extra_calls': 0}

{'function_name': 'forward', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 165, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.module.WeightedFeatureFusion'>", 'shape': None}, 'x': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 64, 160, 208])}, 'outputs': {'type': "<class 'list'>", 'shape': (4,)}}, 'lines': ['if self.weight:', 'nx = x.shape[1]  # input channels', 'for i in range(self.n - 1):', 'a = outputs[self.layers[i]] * w[i + 1] if self.weight else outputs[self.layers[i]]  # feature to add', 'na = a.shape[1]  # feature channels', 'if nx == na:  # same shape', 'x = x + a', 'return x'], 'executed_lines': {194, 176, 181, 182, 183, 184, 187, 188}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 64, 160, 208])}}

{'function_name': 'forward_once', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 338, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}, 'x': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 3, 320, 416])}, 'augment': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['img_size = x.shape[-2:]  # height, width', 'yolo_out, out = [], []', 'batch_size = x.shape[0]', 'scale_factor = [0.83, 0.67]', 'if augment:', 'for i, module in enumerate(self.module_list):', 'name = module.__class__.__name__', 'if name == "WeightedFeatureFusion":', 'elif name == "FeatureConcat":', 'elif name == "YOLOLayer":', 'x = module(x)', 'out.append(x if self.routs[i] else [])', 'x = module(x, out)', 'if self.weight:', 'nx = x.shape[1]  # input channels', 'for i in range(self.n - 1):', 'a = outputs[self.layers[i]] * w[i + 1] if self.weight else outputs[self.layers[i]]  # feature to add', 'na = a.shape[1]  # feature channels', 'if nx == na:  # same shape', 'x = x + a', 'return x', 'yolo_out.append(module(x))'], 'executed_lines': {176, 181, 182, 183, 184, 187, 188, 194, 342, 343, 346, 347, 350, 353, 354, 355, 356, 357, 359, 360, 362, 364}, 'executed_function_lines': {256, 165}, 'extra_calls': 0, 'return_value': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 1024, 10, 13])}}

{'function_name': 'forward', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 256, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.module.YOLOLayer'>", 'shape': None}, 'p': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 255, 10, 13])}}, 'lines': ['if self.onnx_export:', 'bs, _, ny, nx = p.shape  # bs, 255, 13, 13', 'if (self.nx, self.ny) != (nx, ny):', 'self.create_grids((nx, ny), p.device)'], 'executed_lines': {265, 258, 266, 262}, 'executed_function_lines': {241}, 'extra_calls': 0}

{'function_name': 'create_grids', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 241, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.module.YOLOLayer'>", 'shape': None}, 'ng': {'type': "<class 'tuple'>", 'shape': (2,)}, 'device': {'type': "<class 'torch.device'>", 'shape': None}}, 'lines': ['self.nx, self.ny = ng', 'self.ng = torch.tensor(ng, dtype=torch.float, device=device)', 'if not self.training:', 'yv, xv = torch.meshgrid([torch.arange(self.ny, device=device),', 'torch.arange(self.nx, device=device)],', 'indexing="ij")', 'self.grid = torch.stack((xv, yv), 2).view((1, 1, self.ny, self.nx, 2)).float()', 'if self.anchor_vec.device != device:'], 'executed_lines': {242, 243, 246, 247, 248, 249, 250, 252}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'forward', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 256, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.module.YOLOLayer'>", 'shape': None}, 'p': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 255, 10, 13])}}, 'lines': ['if self.onnx_export:', 'bs, _, ny, nx = p.shape  # bs, 255, 13, 13', 'if (self.nx, self.ny) != (nx, ny):', 'self.create_grids((nx, ny), p.device)', 'p = p.view(bs, self.na, self.num_classes_output, self.ny, self.nx)', 'p = p.permute(0, 1, 3, 4, 2).contiguous()  # prediction', 'if self.training:', 'elif self.onnx_export:', 'io = p.clone()  # inference output', 'io[..., :2] = torch.sigmoid(io[..., :2]) + self.grid  # xy', 'io[..., 2:4] = torch.exp(io[..., 2:4]) * self.anchor_wh  # wh yolo method', 'io[..., :4] *= self.stride', 'torch.sigmoid_(io[..., 4:])', 'return io.view(bs, -1, self.num_classes_output), p  # view [1, 3, 13, 13, 85] as [1, 507, 85]'], 'executed_lines': {288, 289, 258, 290, 291, 292, 262, 293, 265, 266, 269, 270, 272, 274}, 'executed_function_lines': {241}, 'extra_calls': 0, 'return_value': {'type': "<class 'tuple'>", 'shape': (2,)}}

{'function_name': 'forward_once', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 338, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}, 'x': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 3, 320, 416])}, 'augment': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['img_size = x.shape[-2:]  # height, width', 'yolo_out, out = [], []', 'batch_size = x.shape[0]', 'scale_factor = [0.83, 0.67]', 'if augment:', 'for i, module in enumerate(self.module_list):', 'name = module.__class__.__name__', 'if name == "WeightedFeatureFusion":', 'elif name == "FeatureConcat":', 'elif name == "YOLOLayer":', 'x = module(x)', 'out.append(x if self.routs[i] else [])', 'x = module(x, out)', 'if self.weight:', 'nx = x.shape[1]  # input channels', 'for i in range(self.n - 1):', 'a = outputs[self.layers[i]] * w[i + 1] if self.weight else outputs[self.layers[i]]  # feature to add', 'na = a.shape[1]  # feature channels', 'if nx == na:  # same shape', 'x = x + a', 'return x', 'yolo_out.append(module(x))', 'x = module(out)'], 'executed_lines': {176, 181, 182, 183, 184, 187, 188, 194, 342, 343, 346, 347, 350, 353, 354, 355, 356, 357, 358, 359, 360, 362, 364}, 'executed_function_lines': {256, 165, 38}, 'extra_calls': 0, 'return_value': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 1024, 10, 13])}}

{'function_name': 'forward', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 38, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.module.FeatureConcat'>", 'shape': None}, 'x': {'type': "<class 'list'>", 'shape': (83,)}}, 'lines': ['if self.multiple:', 'x = x[self.layers[0]]', 'return x'], 'executed_lines': {42, 44, 39}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 512, 10, 13])}}

{'function_name': 'forward_once', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 338, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}, 'x': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 3, 320, 416])}, 'augment': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['img_size = x.shape[-2:]  # height, width', 'yolo_out, out = [], []', 'batch_size = x.shape[0]', 'scale_factor = [0.83, 0.67]', 'if augment:', 'for i, module in enumerate(self.module_list):', 'name = module.__class__.__name__', 'if name == "WeightedFeatureFusion":', 'elif name == "FeatureConcat":', 'elif name == "YOLOLayer":', 'x = module(x)', 'out.append(x if self.routs[i] else [])', 'x = module(x, out)', 'if self.weight:', 'nx = x.shape[1]  # input channels', 'for i in range(self.n - 1):', 'a = outputs[self.layers[i]] * w[i + 1] if self.weight else outputs[self.layers[i]]  # feature to add', 'na = a.shape[1]  # feature channels', 'if nx == na:  # same shape', 'x = x + a', 'return x', 'yolo_out.append(module(x))', 'x = module(out)', 'if self.multiple:', 'x = torch.cat([x[i] for i in self.layers], dim=1)', 'return x', 'if self.onnx_export:', 'bs, _, ny, nx = p.shape  # bs, 255, 13, 13', 'if (self.nx, self.ny) != (nx, ny):', 'self.create_grids((nx, ny), p.device)'], 'executed_lines': {258, 262, 265, 266, 39, 40, 44, 176, 181, 182, 183, 184, 187, 188, 194, 342, 343, 346, 347, 350, 353, 354, 355, 356, 357, 358, 359, 360, 362, 364}, 'executed_function_lines': {256, 241, 165, 38}, 'extra_calls': 1, 'return_value': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 768, 20, 26])}}

{'function_name': 'create_grids', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/module.py', 'lineno': 241, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.module.YOLOLayer'>", 'shape': None}, 'ng': {'type': "<class 'tuple'>", 'shape': (2,)}, 'device': {'type': "<class 'torch.device'>", 'shape': None}}, 'lines': ['self.nx, self.ny = ng', 'self.ng = torch.tensor(ng, dtype=torch.float, device=device)', 'if not self.training:', 'yv, xv = torch.meshgrid([torch.arange(self.ny, device=device),', 'torch.arange(self.nx, device=device)],', 'indexing="ij")', 'self.grid = torch.stack((xv, yv), 2).view((1, 1, self.ny, self.nx, 2)).float()', 'if self.anchor_vec.device != device:'], 'executed_lines': {242, 243, 246, 247, 248, 249, 250, 252}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'forward_once', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 338, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}, 'x': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 3, 320, 416])}, 'augment': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['img_size = x.shape[-2:]  # height, width', 'yolo_out, out = [], []', 'batch_size = x.shape[0]', 'scale_factor = [0.83, 0.67]', 'if augment:', 'for i, module in enumerate(self.module_list):', 'name = module.__class__.__name__', 'if name == "WeightedFeatureFusion":', 'elif name == "FeatureConcat":', 'elif name == "YOLOLayer":', 'x = module(x)', 'out.append(x if self.routs[i] else [])', 'x = module(x, out)', 'if self.weight:', 'nx = x.shape[1]  # input channels', 'for i in range(self.n - 1):', 'a = outputs[self.layers[i]] * w[i + 1] if self.weight else outputs[self.layers[i]]  # feature to add', 'na = a.shape[1]  # feature channels', 'if nx == na:  # same shape', 'x = x + a', 'return x', 'yolo_out.append(module(x))', 'x = module(out)', 'if self.multiple:', 'x = torch.cat([x[i] for i in self.layers], dim=1)', 'return x', 'if self.onnx_export:', 'bs, _, ny, nx = p.shape  # bs, 255, 13, 13', 'if (self.nx, self.ny) != (nx, ny):', 'self.create_grids((nx, ny), p.device)', 'p = p.view(bs, self.na, self.num_classes_output, self.ny, self.nx)', 'p = p.permute(0, 1, 3, 4, 2).contiguous()  # prediction', 'if self.training:', 'elif self.onnx_export:', 'io = p.clone()  # inference output', 'io[..., :2] = torch.sigmoid(io[..., :2]) + self.grid  # xy', 'io[..., 2:4] = torch.exp(io[..., 2:4]) * self.anchor_wh  # wh yolo method', 'io[..., :4] *= self.stride', 'torch.sigmoid_(io[..., 4:])', 'return io.view(bs, -1, self.num_classes_output), p  # view [1, 3, 13, 13, 85] as [1, 507, 85]', 'x = x[self.layers[0]]', 'self.nx, self.ny = ng', 'self.ng = torch.tensor(ng, dtype=torch.float, device=device)', 'if not self.training:', 'yv, xv = torch.meshgrid([torch.arange(self.ny, device=device),', 'torch.arange(self.nx, device=device)],', 'indexing="ij")', 'self.grid = torch.stack((xv, yv), 2).view((1, 1, self.ny, self.nx, 2)).float()', 'if self.anchor_vec.device != device:', 'if self.training:  # train', 'elif self.onnx_export:  # export', 'x, p = zip(*yolo_out)  # inference output, training output', 'x = torch.cat(x, 1)  # cat yolo outputs', 'if augment:  # de-augment results', 'return x, p'], 'executed_lines': {258, 262, 265, 266, 269, 270, 272, 274, 288, 289, 290, 291, 292, 293, 39, 40, 42, 44, 176, 181, 182, 183, 184, 187, 188, 194, 342, 343, 380, 346, 347, 350, 353, 354, 355, 356, 357, 358, 359, 360, 362, 364, 366, 368, 242, 243, 372, 373, 246, 247, 248, 249, 250, 374, 252}, 'executed_function_lines': {256, 241, 165, 38}, 'extra_calls': 0, 'return_value': {'type': "<class 'tuple'>", 'shape': (2,)}}

{'function_name': 'forward', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/models/darknet.py', 'lineno': 317, 'args': {'self': {'type': "<class 'yolov3_pytorch.models.darknet.Darknet'>", 'shape': None}, 'x': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 3, 320, 416])}, 'augment': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['if not augment:', 'return self.forward_once(x)'], 'executed_lines': {322, 323}, 'executed_function_lines': {338}, 'extra_calls': 0, 'return_value': {'type': "<class 'tuple'>", 'shape': (2,)}}

{'function_name': 'inference', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 109, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}}, 'lines': ['print("here")', 'self.model.eval()', 'for path, img, raw_img, video_capture in self.dataset:', 'img = img.to(self.device)', 'img = img.half() if self.half else img.float()', 'img /= 255.0', 'if img.ndimension() == 3:', 'img = img.unsqueeze(0)', 'with torch.no_grad():', 'output, _ = self.model(img, self.augment)', 'if self.half:', 'output = non_max_suppression(output,', 'self.conf_thresh,', 'self.iou_thresh,', 'False,', 'self.filter_classes,', 'self.agnostic_nms)'], 'executed_lines': {131, 132, 133, 134, 135, 136, 110, 111, 113, 115, 116, 117, 119, 120, 123, 124, 127}, 'executed_function_lines': {130, 317, 28, 125}, 'extra_calls': 0}

{'function_name': 'non_max_suppression', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/nms.py', 'lineno': 28, 'args': {'prediction': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 8190, 85])}, 'conf_thresh': {'type': "<class 'float'>", 'shape': None}, 'iou_thresh': {'type': "<class 'float'>", 'shape': None}, 'multi_label': {'type': "<class 'bool'>", 'shape': None}, 'filter_classes': {'type': "<class 'NoneType'>", 'shape': None}, 'agnostic': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['merge = True', 'min_wh, max_wh = 2, 4096', 'timeout = 3.0', 'start_time = time.time()', 'num_classes = prediction[0].shape[1] - 5', 'multi_label &= num_classes > 1', 'output = [None] * prediction.shape[0]', 'for img_idx, x in enumerate(prediction):', 'x = x[x[:, 4] > conf_thresh]  # Confidence threshold', 'x = x[((x[:, 2:4] > min_wh) & (x[:, 2:4] < max_wh)).all(1)]  # Width-height constraints', 'if not x.shape[0]:', 'x[..., 5:] *= x[..., 4:5]  # conf = obj_conf * cls_conf', 'box = xywh2xyxy(x[:, :4])'], 'executed_lines': {64, 68, 71, 44, 46, 48, 50, 53, 55, 56, 58, 60, 61}, 'executed_function_lines': {149}, 'extra_calls': 0}

{'function_name': 'xywh2xyxy', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/common.py', 'lineno': 149, 'args': {'x': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([8190, 4])}}, 'lines': ['y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)', 'y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x', 'y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y', 'y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x', 'y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y', 'return y'], 'executed_lines': {160, 161, 162, 163, 158, 159}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([8190, 4])}}

{'function_name': 'non_max_suppression', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/nms.py', 'lineno': 28, 'args': {'prediction': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([1, 8190, 85])}, 'conf_thresh': {'type': "<class 'float'>", 'shape': None}, 'iou_thresh': {'type': "<class 'float'>", 'shape': None}, 'multi_label': {'type': "<class 'bool'>", 'shape': None}, 'filter_classes': {'type': "<class 'NoneType'>", 'shape': None}, 'agnostic': {'type': "<class 'bool'>", 'shape': None}}, 'lines': ['merge = True', 'min_wh, max_wh = 2, 4096', 'timeout = 3.0', 'start_time = time.time()', 'num_classes = prediction[0].shape[1] - 5', 'multi_label &= num_classes > 1', 'output = [None] * prediction.shape[0]', 'for img_idx, x in enumerate(prediction):', 'x = x[x[:, 4] > conf_thresh]  # Confidence threshold', 'x = x[((x[:, 2:4] > min_wh) & (x[:, 2:4] < max_wh)).all(1)]  # Width-height constraints', 'if not x.shape[0]:', 'x[..., 5:] *= x[..., 4:5]  # conf = obj_conf * cls_conf', 'box = xywh2xyxy(x[:, :4])', 'if multi_label:', 'conf, j = x[:, 5:].max(1)', 'x = torch.cat((box, conf.unsqueeze(1), j.float().unsqueeze(1)), 1)[conf > conf_thresh]', 'if filter_classes:', 'num_boxes = x.shape[0]  # Number of boxes', 'if not num_boxes:', 'classes = x[:, 5] * 0 if agnostic else x[:, 5]', 'boxes, scores = x[:, :4].clone() + classes.view(-1, 1) * max_wh, x[:, 4]  # Adjusted boxes (offset by class), scores', 'i = torchvision.ops.boxes.nms(boxes, scores, iou_thresh)', 'if merge and (1 < num_boxes < 3E3):', 'output[img_idx] = x[i]  # Store the selected detections in the output list', 'if (time.time() - start_time) > timeout:', 'return output'], 'executed_lines': {44, 46, 48, 50, 53, 55, 56, 58, 60, 61, 64, 68, 71, 74, 78, 79, 82, 86, 87, 91, 92, 93, 96, 105, 107, 110}, 'executed_function_lines': {149}, 'extra_calls': 0, 'return_value': {'type': "<class 'list'>", 'shape': (1,)}}

{'function_name': 'inference', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 109, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}}, 'lines': ['print("here")', 'self.model.eval()', 'for path, img, raw_img, video_capture in self.dataset:', 'img = img.to(self.device)', 'img = img.half() if self.half else img.float()', 'img /= 255.0', 'if img.ndimension() == 3:', 'img = img.unsqueeze(0)', 'with torch.no_grad():', 'output, _ = self.model(img, self.augment)', 'if self.half:', 'output = non_max_suppression(output,', 'self.conf_thresh,', 'self.iou_thresh,', 'False,', 'self.filter_classes,', 'self.agnostic_nms)', 'for detect_index, detect_result in enumerate(output):', 'if self.detect_video:', 'elif self.detect_image:', 'path, results, raw_frame = path, "", raw_img', 'save_path = str(Path(self.output) / Path(path).name)', 'results += f"{img.shape[2]}x{img.shape[3]} "', 'gn = torch.tensor(raw_frame.shape)[[1, 0, 1, 0]]', 'if detect_result is not None and len(detect_result):', 'detect_result[:, :4] = scale_coords(img.shape[2:], detect_result[:, :4], raw_frame.shape).round()'], 'executed_lines': {131, 132, 133, 134, 135, 136, 139, 141, 143, 144, 148, 149, 150, 152, 154, 110, 111, 113, 115, 116, 117, 119, 120, 123, 124, 127}, 'executed_function_lines': {130, 125, 118, 28, 317}, 'extra_calls': 0}

{'function_name': 'scale_coords', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/common.py', 'lineno': 118, 'args': {'new_image_shape': {'type': "<class 'torch.Size'>", 'shape': (2,)}, 'coords': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([4542, 4])}, 'raw_image_shape': {'type': "<class 'tuple'>", 'shape': (3,)}, 'ratio_pad': {'type': "<class 'NoneType'>", 'shape': None}}, 'lines': ['if ratio_pad is None:  # calculate from img0_shape', 'gain = max(new_image_shape) / max(raw_image_shape)', '(new_image_shape[1] - raw_image_shape[1] * gain) / 2,', '(new_image_shape[0] - raw_image_shape[0] * gain) / 2', 'pad = (', 'coords[:, [0, 2]] -= pad[0]  # x padding', 'coords[:, [1, 3]] -= pad[1]  # y padding', 'coords[:, :4] /= gain', 'clip_coords(coords, raw_image_shape)'], 'executed_lines': {130, 132, 134, 135, 136, 142, 143, 144, 145}, 'executed_function_lines': {32}, 'extra_calls': 0}

{'function_name': 'clip_coords', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/common.py', 'lineno': 32, 'args': {'boxes': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([4542, 4])}, 'image_shape': {'type': "<class 'tuple'>", 'shape': (3,)}}, 'lines': ['boxes[:, 0].clamp_(0, image_shape[1])  # x1', 'boxes[:, 1].clamp_(0, image_shape[0])  # y1', 'boxes[:, 2].clamp_(0, image_shape[1])  # x2', 'boxes[:, 3].clamp_(0, image_shape[0])  # y2', 'return boxes'], 'executed_lines': {42, 43, 44, 45, 46}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([4542, 4])}}

{'function_name': 'scale_coords', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/common.py', 'lineno': 118, 'args': {'new_image_shape': {'type': "<class 'torch.Size'>", 'shape': (2,)}, 'coords': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([4542, 4])}, 'raw_image_shape': {'type': "<class 'tuple'>", 'shape': (3,)}, 'ratio_pad': {'type': "<class 'NoneType'>", 'shape': None}}, 'lines': ['if ratio_pad is None:  # calculate from img0_shape', 'gain = max(new_image_shape) / max(raw_image_shape)', '(new_image_shape[1] - raw_image_shape[1] * gain) / 2,', '(new_image_shape[0] - raw_image_shape[0] * gain) / 2', 'pad = (', 'coords[:, [0, 2]] -= pad[0]  # x padding', 'coords[:, [1, 3]] -= pad[1]  # y padding', 'coords[:, :4] /= gain', 'clip_coords(coords, raw_image_shape)', 'return coords'], 'executed_lines': {130, 132, 134, 135, 136, 142, 143, 144, 145, 146}, 'executed_function_lines': {32}, 'extra_calls': 0, 'return_value': {'type': "<class 'torch.Tensor'>", 'shape': torch.Size([4542, 4])}}

{'function_name': 'inference', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 109, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}}, 'lines': ['print("here")', 'self.model.eval()', 'for path, img, raw_img, video_capture in self.dataset:', 'img = img.to(self.device)', 'img = img.half() if self.half else img.float()', 'img /= 255.0', 'if img.ndimension() == 3:', 'img = img.unsqueeze(0)', 'with torch.no_grad():', 'output, _ = self.model(img, self.augment)', 'if self.half:', 'output = non_max_suppression(output,', 'self.conf_thresh,', 'self.iou_thresh,', 'False,', 'self.filter_classes,', 'self.agnostic_nms)', 'for detect_index, detect_result in enumerate(output):', 'if self.detect_video:', 'elif self.detect_image:', 'path, results, raw_frame = path, "", raw_img', 'save_path = str(Path(self.output) / Path(path).name)', 'results += f"{img.shape[2]}x{img.shape[3]} "', 'gn = torch.tensor(raw_frame.shape)[[1, 0, 1, 0]]', 'if detect_result is not None and len(detect_result):', 'detect_result[:, :4] = scale_coords(img.shape[2:], detect_result[:, :4], raw_frame.shape).round()', 'for c in detect_result[:, -1].unique():', 'number = (detect_result[:, -1] == c).sum()  # detections per class', 'results += f"{number} {self.class_names[int(c)]}, "', 'for *xyxy, confidence, classes in reversed(detect_result):', 'if self.save_txt:  # Write to file', 'if self.save_image or self.show_image:  # Add bbox to image', 'label = f"{self.class_names[int(classes)]} {confidence:.2f}"', 'plot_one_box(xyxy, raw_frame, label=label, color=self.colors[int(classes)])'], 'executed_lines': {131, 132, 133, 134, 135, 136, 139, 141, 143, 144, 148, 149, 150, 152, 154, 157, 158, 159, 162, 163, 168, 169, 170, 110, 111, 113, 115, 116, 117, 119, 120, 123, 124, 127}, 'executed_function_lines': {130, 125, 118, 28, 317, 31}, 'extra_calls': 0}

{'function_name': 'plot_one_box', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/utils/plots.py', 'lineno': 31, 'args': {'xyxy': {'type': "<class 'list'>", 'shape': (4,)}, 'img': {'type': "<class 'numpy.ndarray'>", 'shape': (576, 768, 3)}, 'color': {'type': "<class 'list'>", 'shape': (3,)}, 'label': {'type': "<class 'str'>", 'shape': (17,)}, 'line_thickness': {'type': "<class 'NoneType'>", 'shape': None}}, 'lines': ['tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1', 'color = color or [random.randint(0, 255) for _ in range(3)]', 'c1, c2 = (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3]))', 'cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)', 'if label:', 'tf = max(tl - 1, 1)', 't_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]', 'c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3', 'cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)', 'cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)'], 'executed_lines': {65, 68, 71, 74, 49, 52, 55, 58, 60, 62}, 'executed_function_lines': set(), 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'inference', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/yolov3_pytorch/engine/inferencer.py', 'lineno': 109, 'args': {'self': {'type': "<class 'yolov3_pytorch.engine.inferencer.Inferencer'>", 'shape': None}}, 'lines': ['print("here")', 'self.model.eval()', 'for path, img, raw_img, video_capture in self.dataset:', 'img = img.to(self.device)', 'img = img.half() if self.half else img.float()', 'img /= 255.0', 'if img.ndimension() == 3:', 'img = img.unsqueeze(0)', 'with torch.no_grad():', 'output, _ = self.model(img, self.augment)', 'if self.half:', 'output = non_max_suppression(output,', 'self.conf_thresh,', 'self.iou_thresh,', 'False,', 'self.filter_classes,', 'self.agnostic_nms)', 'for detect_index, detect_result in enumerate(output):', 'if self.detect_video:', 'elif self.detect_image:', 'path, results, raw_frame = path, "", raw_img', 'save_path = str(Path(self.output) / Path(path).name)', 'results += f"{img.shape[2]}x{img.shape[3]} "', 'gn = torch.tensor(raw_frame.shape)[[1, 0, 1, 0]]', 'if detect_result is not None and len(detect_result):', 'detect_result[:, :4] = scale_coords(img.shape[2:], detect_result[:, :4], raw_frame.shape).round()', 'for c in detect_result[:, -1].unique():', 'number = (detect_result[:, -1] == c).sum()  # detections per class', 'results += f"{number} {self.class_names[int(c)]}, "', 'for *xyxy, confidence, classes in reversed(detect_result):', 'if self.save_txt:  # Write to file', 'if self.save_image or self.show_image:  # Add bbox to image', 'label = f"{self.class_names[int(classes)]} {confidence:.2f}"', 'plot_one_box(xyxy, raw_frame, label=label, color=self.colors[int(classes)])', 'tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1', 'color = color or [random.randint(0, 255) for _ in range(3)]', 'c1, c2 = (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3]))', 'cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)', 'if label:', 'tf = max(tl - 1, 1)', 't_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]', 'c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3', 'cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)', 'cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)', 'print(results)', 'if self.show_image:', 'if self.save_image:', 'if self.dataset.mode == "images":', 'cv2.imwrite(save_path, raw_frame)'], 'executed_lines': {131, 132, 133, 134, 135, 136, 139, 141, 143, 144, 148, 149, 150, 152, 154, 157, 158, 159, 162, 163, 168, 169, 170, 173, 176, 49, 52, 182, 55, 183, 185, 58, 60, 62, 65, 68, 71, 74, 110, 111, 113, 115, 116, 117, 119, 120, 123, 124, 127}, 'executed_function_lines': {130, 125, 118, 28, 317, 31}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': 'main', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/tools/inference.py', 'lineno': 129, 'args': {}, 'lines': ['opts = get_opts()', 'app = Inferencer(opts)', 'app.inference()'], 'executed_lines': {130, 132, 133}, 'executed_function_lines': {109, 22, 31}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

{'function_name': '<module>', 'filename': '/media/sarthak/storage/Pipeline_Generator/YOLOv3-PyTorch/tools/inference.py', 'lineno': 0, 'args': {}, 'lines': ['"""', 'import argparse', 'from yolov3_pytorch.engine.inferencer import Inferencer', 'def get_opts() -> argparse.Namespace:', 'def main() -> None:', 'if __name__ == "__main__":', 'main()'], 'executed_lines': {129, 136, 137, 14, 17, 19, 22}, 'executed_function_lines': {0, 129}, 'extra_calls': 0, 'return_value': {'type': "<class 'NoneType'>", 'shape': None}}

